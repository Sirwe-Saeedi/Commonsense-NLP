{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoBERTa_Train+trial_test_as_ValidSet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "EjbM-Sygo5Qi",
        "p1IUoK5qo-ti",
        "nSU7yERLP_66",
        "4JrUHXms16cn",
        "3HtOdje3AjBQ",
        "xM3hk7x3xig9",
        "O7Y7KQWLyGci",
        "xVhGfoEjXZ3H",
        "1-G03mmwH3aI",
        "mkyubuJSOzg3",
        "GfjYoa6WmkN6",
        "YUmsUOIv8EUO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjbM-Sygo5Qi",
        "colab_type": "text"
      },
      "source": [
        "#Importing files from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRt6QJMzn6XV",
        "colab_type": "code",
        "outputId": "96d995ed-ba60-4f1c-bf21-9a4aaabdc285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.7.2)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (45.1.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lqWFNsNldfd",
        "colab_type": "code",
        "outputId": "1f2609e4-a3c5-4d31-e6a4-70ed846a89eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Oai4t8NoAcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcDTGVo0oFUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu1dM2gtoUdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"16_ZNThSbAQyY1Hw6CXBm6hteZ0eeLJpI\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('pytorch_model.bin')        # replace the file name with your file\n",
        "downloaded = drive.CreateFile({'id':\"1ill09R5sdg7GzBCfKfGty8eW5F9YZ-m1\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('config.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1IUoK5qo-ti",
        "colab_type": "text"
      },
      "source": [
        "#Code started"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi9J5cpfd3tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers==2.4.1 -q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEs9JVE3vj6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import *\n",
        "\n",
        "model_name = 'roberta-large' #uncased should have do_lower_case=True\n",
        "model = AutoModelForSequenceClassification.from_pretrained('./')\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name, do_lower_case=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlT8sfnJ5h3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "cab8a429-38e0-46fb-bc38-d3b75062e6af"
      },
      "source": [
        "model.classifier"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaClassificationHead(\n",
              "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnUdn5NA3Xc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models_names = [ \n",
        "'bert-base-uncased',\n",
        "'bert-large-uncased',\n",
        "'bert-base-cased',\n",
        "'bert-large-cased',\n",
        "'bert-base-multilingual-uncased',\n",
        "'bert-base-multilingual-cased',\n",
        "'bert-large-uncased-whole-word-masking',\n",
        "'bert-large-cased-whole-word-masking',\n",
        "'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
        "'bert-large-cased-whole-word-masking-finetuned-squad',\n",
        "'bert-base-cased-finetuned-mrpc',\n",
        "'albert-base-v1',\n",
        "'albert-large-v1',\n",
        "'albert-xlarge-v1',\n",
        "'albert-xxlarge-v1',\n",
        "'albert-base-v2',\n",
        "'albert-large-v2',\n",
        "'albert-xlarge-v2',\n",
        "'albert-xxlarge-v2',\n",
        "'xlm-mlm-en-2048',\n",
        "'xlm-mlm-ende-1024',\n",
        "'xlm-mlm-enfr-1024',\n",
        "'xlm-mlm-enro-1024',\n",
        "'xlm-mlm-xnli15-1024',\n",
        "'xlm-mlm-tlm-xnli15-1024',\n",
        "'xlm-clm-enfr-1024',\n",
        "'xlm-clm-ende-1024',\n",
        "'xlm-mlm-17-1280',\n",
        "'xlm-mlm-100-1280',\n",
        "'xlm-roberta-base',\n",
        "'xlm-roberta-large',\n",
        "'roberta-base',\n",
        "'roberta-large',\n",
        "'roberta-large-mnli',\n",
        "'roberta-base-openai-detector',\n",
        "'roberta-large-openai-detector',\n",
        "'xlnet-base-cased',\n",
        "'xlnet-large-cased',\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKOTlwcmxmej",
        "colab_type": "text"
      },
      "source": [
        "# RoBERTa Fine-Tuning Tutorial with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_ZDhicpHkV",
        "colab_type": "text"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSU7yERLP_66",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Using Colab GPU for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfSbAA4QHas",
        "colab_type": "code",
        "outputId": "35cf0565-9d15-4361-f60d-403b13cce3e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYsV4H8fCpZ-",
        "colab_type": "code",
        "outputId": "9bd8f9a6-2c46-4046-9e01-5b3defaf7d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ElsnSNUridI",
        "colab_type": "text"
      },
      "source": [
        "## 1.2. Installing the Hugging Face Library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guw6ZNtaswKc",
        "colab_type": "text"
      },
      "source": [
        "# 2. Loading SemEval Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JrUHXms16cn",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. Download & Extract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZNVW6xd0T0X",
        "colab_type": "text"
      },
      "source": [
        "We'll use the `wget` package to download the dataset to the Colab instance's file system. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m6AnuFv0QXQ",
        "colab_type": "code",
        "outputId": "b37cb32e-f71d-4aeb-dd40-24172ff4c3af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08pO03Ff1BjI",
        "colab_type": "text"
      },
      "source": [
        "The dataset is hosted on GitHub in this repo: https://nyu-mll.github.io/CoLA/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMtmPMkBzrvs",
        "colab_type": "code",
        "outputId": "443f137c-1cf7-441f-cdf2-fd0ea280c831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "!wget -q https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_data_all.csv\n",
        "!wget -q https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_answers_all.csv\n",
        "\n",
        "#trial\n",
        "!wget -q https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Trial%20Data/taskA_trial_data.csv\n",
        "!wget -q https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Trial%20Data/taskA_trial_answer.csv\n",
        "#dev\n",
        "!wget https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Dev%20Data/subtaskA_gold_answers.csv\n",
        "!wget https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Dev%20Data/subtaskA_dev_data.csv\n",
        "\n",
        "#test\n",
        "!wget https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Test%20Data/subtaskA_test_data.csv"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n",
            "--2020-02-21 18:05:32--  https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Dev%20Data/subtaskA_gold_answers.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6443 (6.3K) [text/plain]\n",
            "Saving to: ‘subtaskA_gold_answers.csv.1’\n",
            "\n",
            "subtaskA_gold_answe 100%[===================>]   6.29K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-02-21 18:05:33 (156 MB/s) - ‘subtaskA_gold_answers.csv.1’ saved [6443/6443]\n",
            "\n",
            "--2020-02-21 18:05:33--  https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Dev%20Data/subtaskA_dev_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77541 (76K) [text/plain]\n",
            "Saving to: ‘subtaskA_dev_data.csv.1’\n",
            "\n",
            "subtaskA_dev_data.c 100%[===================>]  75.72K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2020-02-21 18:05:34 (9.52 MB/s) - ‘subtaskA_dev_data.csv.1’ saved [77541/77541]\n",
            "\n",
            "--2020-02-21 18:05:34--  https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Test%20Data/subtaskA_test_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 79701 (78K) [text/plain]\n",
            "Saving to: ‘subtaskA_test_data.csv.1’\n",
            "\n",
            "subtaskA_test_data. 100%[===================>]  77.83K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2020-02-21 18:05:35 (8.72 MB/s) - ‘subtaskA_test_data.csv.1’ saved [79701/79701]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQUy9Tat2EF_",
        "colab_type": "text"
      },
      "source": [
        "## 2.2. Parse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeyVCXT31EZQ",
        "colab_type": "text"
      },
      "source": [
        "We can see from the file names that both `tokenized` and `raw` versions of the data are available. \n",
        "\n",
        "We can't use the pre-tokenized version because, in order to apply the pre-trained BERT, we *must* use the tokenizer provided by the model. This is because (1) the model has a specific, fixed vocabulary and (2) the BERT tokenizer has a particular way of handling out-of-vocabulary words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYWzeGSY2xh3",
        "colab_type": "text"
      },
      "source": [
        "We'll use pandas to parse the \"in-domain\" training set and look at a few of its properties and data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONNS01p-9GeE",
        "colab_type": "text"
      },
      "source": [
        "##training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cxpwQr6SpGd",
        "colab_type": "code",
        "outputId": "77130e8e-8725-4e0f-f88f-e8245ccc601f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df_data = pd.read_csv('subtaskA_data_all.csv', error_bad_lines=False)\n",
        "df_answers = pd.read_csv('subtaskA_answers_all.csv', error_bad_lines=False, header=None, names=['id', 'label'])\n",
        "\n",
        "df = df_data.set_index('id').join(df_answers.set_index('id'))\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if df['sent0'][i][-1] != '.' or df['sent1'][i][-1] !='.' :\n",
        "      df['sent0'][i] = df['sent0'][i] +'.'\n",
        "      df['sent1'][i] = df['sent1'][i] +'.'\n",
        "      \n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.head(10)\n",
        "df"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 10,000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent0</th>\n",
              "      <th>sent1</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>He poured orange juice on his cereal.</td>\n",
              "      <td>He poured milk on his cereal.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>He drinks apple.</td>\n",
              "      <td>He drinks milk.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jeff ran a mile today.</td>\n",
              "      <td>Jeff ran 100,000 miles today.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A mosquito stings me.</td>\n",
              "      <td>I sting a mosquito.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A niece is a person.</td>\n",
              "      <td>A giraffe is a person.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>Mark ate a big bitter cherry pie.</td>\n",
              "      <td>Mark ate a big sweet cherry pie.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>Gloria wears a cat on her head.</td>\n",
              "      <td>Gloria wears a hat on her head.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>Harry went to the barbershop to have his hair ...</td>\n",
              "      <td>Harry went to the barbershop to have his glass...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>Reilly is sleeping on the couch.</td>\n",
              "      <td>Reilly is sleeping on the window.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>I have a desk on my lamp.</td>\n",
              "      <td>I have a lamp on my desk.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  sent0  ... label\n",
              "0                 He poured orange juice on his cereal.  ...     0\n",
              "1                                      He drinks apple.  ...     0\n",
              "2                                Jeff ran a mile today.  ...     1\n",
              "3                                 A mosquito stings me.  ...     1\n",
              "4                                  A niece is a person.  ...     1\n",
              "...                                                 ...  ...   ...\n",
              "9995                  Mark ate a big bitter cherry pie.  ...     0\n",
              "9996                    Gloria wears a cat on her head.  ...     0\n",
              "9997  Harry went to the barbershop to have his hair ...  ...     1\n",
              "9998                   Reilly is sleeping on the couch.  ...     1\n",
              "9999                          I have a desk on my lamp.  ...     0\n",
              "\n",
              "[10000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYx2mLFL9Kpx",
        "colab_type": "text"
      },
      "source": [
        "##trial set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UkeC7SG2krJ",
        "colab_type": "code",
        "outputId": "988eff24-4ae1-4f77-fd83-77a0ee53a593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df_data = pd.read_csv('taskA_trial_data.csv', error_bad_lines=False)\n",
        "df_answers = pd.read_csv('taskA_trial_answer.csv', error_bad_lines=False, header=None, names=['id', 'label'])\n",
        "\n",
        "df_trial = df_data.set_index('id').join(df_answers.set_index('id'))\n",
        "df_trial.reset_index(drop=True, inplace=True)\n",
        "\n",
        "for i in range(len(df_trial)):\n",
        "    if df_trial['sent0'][i][-1] != '.' or df_trial['sent1'][i][-1] !='.' :\n",
        "      df_trial['sent0'][i] = df_trial['sent0'][i] +'.'\n",
        "      df_trial['sent1'][i] = df_trial['sent1'][i] +'.'\n",
        "      \n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df_trial.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df_trial.head(10)\n",
        "df_trial"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,021\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent0</th>\n",
              "      <th>sent1</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>he put an elephant into the fridge.</td>\n",
              "      <td>he put a turkey into the fridge.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>my sister eats an apple after breakfast every ...</td>\n",
              "      <td>my sister eats a stone after breakfast every day.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>money can be used for buying cars.</td>\n",
              "      <td>money can be used for buying stars.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>New York is located in the northeastern part o...</td>\n",
              "      <td>USA is located in the northeastern part of New...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a man can better see stars and the moon in day...</td>\n",
              "      <td>a man can hardly see stars and the moon in day...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016</th>\n",
              "      <td>she cleans her teeth with toothpaste.</td>\n",
              "      <td>she cleans her teeth with shampoo.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017</th>\n",
              "      <td>you can see a dinosaur in the museum.</td>\n",
              "      <td>you can see a dinosaur in the zoo.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018</th>\n",
              "      <td>people have to hold onto their hats because of...</td>\n",
              "      <td>people have to hold onto their shoes because o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019</th>\n",
              "      <td>Rebecca wakes up because she takes sleeping pi...</td>\n",
              "      <td>Rebecca wakes up because she sets an alarm.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020</th>\n",
              "      <td>people bleed when they feel hot.</td>\n",
              "      <td>people sweat when they feel hot.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2021 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  sent0  ... label\n",
              "0                   he put an elephant into the fridge.  ...     0\n",
              "1     my sister eats an apple after breakfast every ...  ...     1\n",
              "2                    money can be used for buying cars.  ...     1\n",
              "3     New York is located in the northeastern part o...  ...     1\n",
              "4     a man can better see stars and the moon in day...  ...     0\n",
              "...                                                 ...  ...   ...\n",
              "2016              she cleans her teeth with toothpaste.  ...     1\n",
              "2017              you can see a dinosaur in the museum.  ...     1\n",
              "2018  people have to hold onto their hats because of...  ...     1\n",
              "2019  Rebecca wakes up because she takes sleeping pi...  ...     0\n",
              "2020                   people bleed when they feel hot.  ...     0\n",
              "\n",
              "[2021 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HtOdje3AjBQ",
        "colab_type": "text"
      },
      "source": [
        "##training set + trial set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJH7dNDEAmn9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "6d981aa0-7fc3-4b5c-d98c-355d3088864b"
      },
      "source": [
        "frames = [df, df_trial]\n",
        "df = pd.concat(frames, ignore_index=True)\n",
        "df"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent0</th>\n",
              "      <th>sent1</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>He poured orange juice on his cereal.</td>\n",
              "      <td>He poured milk on his cereal.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>He drinks apple.</td>\n",
              "      <td>He drinks milk.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jeff ran a mile today.</td>\n",
              "      <td>Jeff ran 100,000 miles today.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A mosquito stings me.</td>\n",
              "      <td>I sting a mosquito.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A niece is a person.</td>\n",
              "      <td>A giraffe is a person.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12016</th>\n",
              "      <td>she cleans her teeth with toothpaste.</td>\n",
              "      <td>she cleans her teeth with shampoo.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12017</th>\n",
              "      <td>you can see a dinosaur in the museum.</td>\n",
              "      <td>you can see a dinosaur in the zoo.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12018</th>\n",
              "      <td>people have to hold onto their hats because of...</td>\n",
              "      <td>people have to hold onto their shoes because o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12019</th>\n",
              "      <td>Rebecca wakes up because she takes sleeping pi...</td>\n",
              "      <td>Rebecca wakes up because she sets an alarm.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12020</th>\n",
              "      <td>people bleed when they feel hot.</td>\n",
              "      <td>people sweat when they feel hot.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12021 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   sent0  ... label\n",
              "0                  He poured orange juice on his cereal.  ...     0\n",
              "1                                       He drinks apple.  ...     0\n",
              "2                                 Jeff ran a mile today.  ...     1\n",
              "3                                  A mosquito stings me.  ...     1\n",
              "4                                   A niece is a person.  ...     1\n",
              "...                                                  ...  ...   ...\n",
              "12016              she cleans her teeth with toothpaste.  ...     1\n",
              "12017              you can see a dinosaur in the museum.  ...     1\n",
              "12018  people have to hold onto their hats because of...  ...     1\n",
              "12019  Rebecca wakes up because she takes sleeping pi...  ...     0\n",
              "12020                   people bleed when they feel hot.  ...     0\n",
              "\n",
              "[12021 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAqaGXw6-hhG",
        "colab_type": "text"
      },
      "source": [
        "##dev set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo5LVuNh-jnX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "b6f9b159-0fce-4461-a5e7-6c29cbac0242"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df_data = pd.read_csv('subtaskA_dev_data.csv', error_bad_lines=False)\n",
        "df_answers = pd.read_csv('subtaskA_gold_answers.csv', error_bad_lines=False, header=None, names=['id', 'label'])\n",
        "\n",
        "df_dev = df_data.set_index('id').join(df_answers.set_index('id'))\n",
        "df_dev.reset_index(drop=True, inplace=True)\n",
        "\n",
        "for i in range(len(df_dev)):\n",
        "    if df_dev['sent0'][i][-1] != '.' or df_dev['sent1'][i][-1] !='.' :\n",
        "      df_dev['sent0'][i] = df_dev['sent0'][i] +'.'\n",
        "      df_dev['sent1'][i] = df_dev['sent1'][i] +'.'\n",
        "      \n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df_dev.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df_dev.head(10)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 997\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent0</th>\n",
              "      <th>sent1</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Summer in North America is great for skiing,  ...</td>\n",
              "      <td>Summer in North America is great for swimming,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>You can use detergent to dye your hair.</td>\n",
              "      <td>You can use bleach to dye your hair.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>passing your driving license exams requires st...</td>\n",
              "      <td>passing your university exams requires studyin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The hangers bought the closet.</td>\n",
              "      <td>The closet got hangers.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>coffee takes sleep.</td>\n",
              "      <td>coffee depresses people.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The cat used the litter box.</td>\n",
              "      <td>The cat used the toilet.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>He wrote an exam in pen.</td>\n",
              "      <td>He wrote an exam in knife.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>she gave birth to a baby.</td>\n",
              "      <td>he gave birth to a baby.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Chicken are found on a farm.</td>\n",
              "      <td>Chicken can swim in water.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>A stick wants to have sex.</td>\n",
              "      <td>A person wants to have sex.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sent0  ... label\n",
              "0  Summer in North America is great for skiing,  ...  ...     0\n",
              "1            You can use detergent to dye your hair.  ...     0\n",
              "2  passing your driving license exams requires st...  ...     0\n",
              "3                     The hangers bought the closet.  ...     0\n",
              "4                                coffee takes sleep.  ...     1\n",
              "5                       The cat used the litter box.  ...     1\n",
              "6                           He wrote an exam in pen.  ...     1\n",
              "7                          she gave birth to a baby.  ...     1\n",
              "8                       Chicken are found on a farm.  ...     1\n",
              "9                         A stick wants to have sex.  ...     0\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM3hk7x3xig9",
        "colab_type": "text"
      },
      "source": [
        "##test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lq9nHhPxgg-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "bc673dba-c54b-41fd-92a1-97b194afddd0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df_test = pd.read_csv('subtaskA_test_data.csv', error_bad_lines=False)\n",
        "df_test"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sent0</th>\n",
              "      <th>sent1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1175</td>\n",
              "      <td>He loves to stroll at the park with his bed</td>\n",
              "      <td>He loves to stroll at the park with his dog.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>452</td>\n",
              "      <td>The inverter was able to power the continent.</td>\n",
              "      <td>The inverter was able to power the house</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>275</td>\n",
              "      <td>The chef put extra lemons on the pizza.</td>\n",
              "      <td>The chef put extra mushrooms on the pizza.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>869</td>\n",
              "      <td>sugar is used to make coffee sour</td>\n",
              "      <td>sugar is used to make coffee sweet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "      <td>There are beautiful flowers here and there in ...</td>\n",
              "      <td>There are beautiful planes here and there in t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>1114</td>\n",
              "      <td>If it had rained, you would got wet.</td>\n",
              "      <td>If it is a sunny day, you would got wet.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>8</td>\n",
              "      <td>ice hockey is a sport</td>\n",
              "      <td>ice hockey is a financial institution</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>1945</td>\n",
              "      <td>He put water without a container in the freeze...</td>\n",
              "      <td>He put a watermelon in the freezer for 24 hours</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>1053</td>\n",
              "      <td>The desert has sand that you can drink.</td>\n",
              "      <td>The desert is very dry, so bring water when yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1123</td>\n",
              "      <td>My friend runs for 2 inches every day.</td>\n",
              "      <td>My friend runs for 2 miles every day.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ...                                              sent1\n",
              "0    1175  ...       He loves to stroll at the park with his dog.\n",
              "1     452  ...           The inverter was able to power the house\n",
              "2     275  ...         The chef put extra mushrooms on the pizza.\n",
              "3     869  ...                 sugar is used to make coffee sweet\n",
              "4      50  ...  There are beautiful planes here and there in t...\n",
              "..    ...  ...                                                ...\n",
              "995  1114  ...           If it is a sunny day, you would got wet.\n",
              "996     8  ...              ice hockey is a financial institution\n",
              "997  1945  ...    He put a watermelon in the freezer for 24 hours\n",
              "998  1053  ...  The desert is very dry, so bring water when yo...\n",
              "999  1123  ...              My friend runs for 2 miles every day.\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SMZ5T5Imhlx",
        "colab_type": "text"
      },
      "source": [
        "#Let's extract the sentences and labels of our training set as numpy ndarrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuE5BqICAne2",
        "colab_type": "code",
        "outputId": "21140d8d-7ab7-4bd5-e94e-f106dc72d819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentence1 = df.sent1.values\n",
        "sentence0 = df.sent0.values\n",
        "labels = df.label.values\n",
        "\n",
        "\n",
        "\n",
        "#test data\n",
        "#sentence1 = df_test.sent1.values\n",
        "#sentence0 = df_test.sent0.values\n",
        "#test_labels = df_test.label.values\n",
        "\n",
        "\n",
        "#dev\n",
        "sentence1 = df_dev.sent1.values\n",
        "sentence0 = df_dev.sent0.values\n",
        "dev_labels = df_dev.label.values\n",
        "\n",
        "dev_labels"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
              "       1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
              "       1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
              "       1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
              "       1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
              "       0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
              "       1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
              "       1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
              "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
              "       0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
              "       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
              "       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
              "       0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
              "       0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
              "       1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
              "       1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
              "       1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
              "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
              "       1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
              "       0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
              "       0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5O1eV-Pfct",
        "colab_type": "text"
      },
      "source": [
        "# 3. Tokenization & Input Formatting\n",
        "\n",
        "In this section, we'll transform our dataset into the format that BERT can be trained on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viKGCCh8izww",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Required Formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ow__Ujco6HIK"
      },
      "source": [
        "## 3.2. Sentences to IDs using tokenizer.encode()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krA6bqf6pz1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dFJsdh7Y-07",
        "colab_type": "text"
      },
      "source": [
        "##encode_plus for training + trial data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bBdb3pt8LuQ",
        "colab_type": "code",
        "outputId": "9e849261-0e23-47d7-da93-e47c19be99ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "input_type_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for i in range(len(df)):\n",
        "    tokenizer.padding_side = 'right'\n",
        "    encoded_sent = tokenizer.encode_plus(\n",
        "                        df['sent0'][i],            # Sentence to encode.\n",
        "                        df['sent1'][i],\n",
        "                        add_special_tokens = True, \n",
        "                        max_length = MAX_LEN,\n",
        "                        pad_to_max_length = True\n",
        "                        )\n",
        "\n",
        "    #Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent['input_ids'])\n",
        "    input_type_ids.append(encoded_sent['token_type_ids'])\n",
        "    attention_masks.append(encoded_sent['attention_mask'])\n",
        "\n",
        "#Print last sentences of df, now as a list of IDs.\n",
        "print('original: ' , df['sent0'][0], df['sent1'][0])\n",
        "print('Token IDs:', input_ids[10])\n",
        "print('Type IDs:', input_type_ids[10])\n",
        "print('MASK IDs:', attention_masks[10])\n",
        "\n",
        "#Original:  I have a desk on my lamp.\n",
        "#Tokenized:  ['▁', 'I', '▁have', '▁a', '▁desk', '▁on', '▁my', '▁lamp', '.']\n",
        "#Token IDs:  [13, 1, 57, 21, 2911, 27, 51, 6792, 9]\n",
        "tokenizer.cls_token_id, tokenizer.sep_token_id"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:  He poured orange juice on his cereal. He poured milk on his cereal.\n",
            "Token IDs: [0, 717, 6149, 29, 3529, 449, 354, 15, 9274, 4, 2, 2, 32180, 465, 7689, 15, 9274, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "MASK IDs: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4-66pziY7aA",
        "colab_type": "text"
      },
      "source": [
        "##encode_plus for test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkJhHok3YfPL",
        "colab_type": "code",
        "outputId": "1240a3d6-c02f-407c-fdf9-f9c517f01458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "validation_inputs = []\n",
        "validation_type_ids = []\n",
        "validation_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for i in range(len(df_test)):\n",
        "    tokenizer.padding_side = 'right'\n",
        "    encoded_sent = tokenizer.encode_plus(\n",
        "                        df_test['sent0'][i],            # Sentence to encode.\n",
        "                        df_test['sent1'][i],\n",
        "                        add_special_tokens = True, \n",
        "                        max_length = MAX_LEN,\n",
        "                        pad_to_max_length = True\n",
        "                        )\n",
        "\n",
        "    #Add the encoded sentence to the list.\n",
        "    validation_inputs.append(encoded_sent['input_ids'])\n",
        "    validation_type_ids.append(encoded_sent['token_type_ids'])\n",
        "    validation_masks.append(encoded_sent['attention_mask'])\n",
        "\n",
        "#Print last sentences of df, now as a list of IDs.\n",
        "print('original: ' , df_test['sent0'][0], df_test['sent1'][0])\n",
        "print('Token IDs:', validation_inputs[10])\n",
        "print('Type IDs:', validation_type_ids[10])\n",
        "print('MASK IDs:', validation_masks[10])\n",
        "\n",
        "#Original:  I have a desk on my lamp.\n",
        "#Tokenized:  ['▁', 'I', '▁have', '▁a', '▁desk', '▁on', '▁my', '▁lamp', '.']\n",
        "#Token IDs:  [13, 1, 57, 21, 2911, 27, 51, 6792, 9]\n",
        "tokenizer.cls_token_id, tokenizer.sep_token_id"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:  He loves to stroll at the park with his bed He loves to stroll at the park with his dog.\n",
            "Token IDs: [0, 133, 2931, 1326, 66, 69, 2, 2, 2515, 1326, 66, 5, 2931, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "MASK IDs: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRp4O7D295d_",
        "colab_type": "text"
      },
      "source": [
        "## 3.5. Training & Validation Split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVhGfoEjXZ3H",
        "colab_type": "text"
      },
      "source": [
        "##The following code split 10 % of training set as validation **set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFbE-UHvsb7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels, train_type_ids, validation_type_ids, train_masks, validation_masks = (\n",
        "    train_test_split(input_ids, labels, input_type_ids, attention_masks, random_state=2018, test_size=0.1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxF7mNjXXo1T",
        "colab_type": "text"
      },
      "source": [
        "##The following set has all training data for training set and use trial data for validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFXNGWFTX3QA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "train_inputs, train_labels, train_type_ids, train_masks = input_ids, labels , input_type_ids , attention_masks\n",
        "\n",
        "\n",
        "validation_inputs,  validation_type_ids, validation_masks = validation_inputs, validation_type_ids, validation_masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LzSbTqW9_BR",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 3.6. Converting to PyTorch Data Types"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw5K2A5Ko1RF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "train_type_ids = torch.tensor(train_type_ids) \n",
        "validation_type_ids = torch.tensor(validation_type_ids)\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "#validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgLpFVlo1Z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels, train_type_ids)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_type_ids)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bwa6Rts-02-",
        "colab_type": "text"
      },
      "source": [
        "# 4. Train Our Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRWT-D4U_Pvx",
        "colab_type": "text"
      },
      "source": [
        "## 4.2. Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.cuda();\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p0upAhhRiIx",
        "colab_type": "code",
        "outputId": "b0be3599-a774-4c45-9bf1-64c5dfa402cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 20\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "print('len(train_dataloader)', len(train_dataloader))\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "print('total_steps', total_steps)\n",
        "warmup_steps = int(0.06 * total_steps)\n",
        "print('warmup_steps', warmup_steps)\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = warmup_steps, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(train_dataloader) 752\n",
            "total_steps 15040\n",
            "warmup_steps 902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqfmWwUR_Sox",
        "colab_type": "text"
      },
      "source": [
        "## 4.3. Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE5B99H5H2-W",
        "colab_type": "text"
      },
      "source": [
        "###Define a helper function for calculating accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cQNvaZ9bnyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpt6tR83keZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfNIhN19te3N",
        "colab_type": "text"
      },
      "source": [
        "We're ready to kick off the training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_",
        "colab_type": "code",
        "outputId": "9b2de0a9-f049-4300-a637-ef71c75e6073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}    LR . {:.2E}'.format(step, len(train_dataloader), elapsed, scheduler.get_lr()[0]))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        b_type_ids = batch[3].to(device)\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=b_type_ids, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    pred = []\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_type_ids = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=b_type_ids, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        #tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        pred.append(list(np.argmax(logits, axis=1).flatten()))\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        #eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    #print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 20 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of    752.    Elapsed: 0:00:32    LR . 4.43E-07\n",
            "  Batch    80  of    752.    Elapsed: 0:01:03    LR . 8.87E-07\n",
            "  Batch   120  of    752.    Elapsed: 0:01:34    LR . 1.33E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:06    LR . 1.77E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:37    LR . 2.22E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:08    LR . 2.66E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:40    LR . 3.10E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:11    LR . 3.55E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:42    LR . 3.99E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:14    LR . 4.43E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:45    LR . 4.88E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:16    LR . 5.32E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:47    LR . 5.76E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:19    LR . 6.21E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:50    LR . 6.65E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:21    LR . 7.10E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:53    LR . 7.54E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:24    LR . 7.98E-06\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:09:49\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 2 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 8.78E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 9.22E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:34    LR . 9.67E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 9.99E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 9.96E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:08    LR . 9.94E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:39    LR . 9.91E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:10    LR . 9.88E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 9.85E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:13    LR . 9.82E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:44    LR . 9.79E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:15    LR . 9.77E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:46    LR . 9.74E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:18    LR . 9.71E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:49    LR . 9.68E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:20    LR . 9.65E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:52    LR . 9.63E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:23    LR . 9.60E-06\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:09:48\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 3 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 9.55E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:03    LR . 9.52E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:34    LR . 9.49E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 9.46E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 9.43E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 9.40E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:39    LR . 9.38E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:10    LR . 9.35E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 9.32E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 9.29E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:44    LR . 9.26E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:15    LR . 9.23E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:46    LR . 9.21E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:17    LR . 9.18E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 9.15E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:20    LR . 9.12E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:51    LR . 9.09E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:22    LR . 9.06E-06\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:09:47\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 4 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 9.01E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 8.99E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:34    LR . 8.96E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 8.93E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 8.90E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 8.87E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 8.84E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:10    LR . 8.82E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 8.79E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 8.76E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:44    LR . 8.73E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:15    LR . 8.70E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:46    LR . 8.67E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:17    LR . 8.65E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 8.62E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:20    LR . 8.59E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:51    LR . 8.56E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:22    LR . 8.53E-06\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:09:47\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 5 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 8.48E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 8.45E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:34    LR . 8.43E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 8.40E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 8.37E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 8.34E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 8.31E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:10    LR . 8.28E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 8.26E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 8.23E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 8.20E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:15    LR . 8.17E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:46    LR . 8.14E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:17    LR . 8.11E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 8.09E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 8.06E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:51    LR . 8.03E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:22    LR . 8.00E-06\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 6 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 7.95E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 7.92E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:33    LR . 7.89E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 7.87E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 7.84E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 7.81E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 7.78E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:09    LR . 7.75E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 7.72E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 7.70E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 7.67E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:14    LR . 7.64E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:45    LR . 7.61E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:17    LR . 7.58E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 7.55E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 7.53E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:50    LR . 7.50E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:21    LR . 7.47E-06\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 7 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 7.42E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 7.39E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:33    LR . 7.36E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 7.33E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 7.31E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 7.28E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 7.25E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:09    LR . 7.22E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:40    LR . 7.19E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 7.16E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 7.14E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:14    LR . 7.11E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:45    LR . 7.08E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:17    LR . 7.05E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 7.02E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 6.99E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:50    LR . 6.97E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:21    LR . 6.94E-06\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 8 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 6.89E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 6.86E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:33    LR . 6.83E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 6.80E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 6.77E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 6.74E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 6.72E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:09    LR . 6.69E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 6.66E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 6.63E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 6.60E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:14    LR . 6.58E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:45    LR . 6.55E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:16    LR . 6.52E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 6.49E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 6.46E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:50    LR . 6.43E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:21    LR . 6.41E-06\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 9 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 6.35E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 6.33E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:34    LR . 6.30E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 6.27E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 6.24E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 6.21E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 6.18E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:10    LR . 6.16E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 6.13E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 6.10E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 6.07E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:14    LR . 6.04E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:46    LR . 6.01E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:17    LR . 5.99E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 5.96E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 5.93E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:51    LR . 5.90E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:22    LR . 5.87E-06\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 10 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 5.82E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 5.79E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:34    LR . 5.77E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 5.74E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 5.71E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 5.68E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 5.65E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:09    LR . 5.62E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:40    LR . 5.60E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 5.57E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 5.54E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:14    LR . 5.51E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:45    LR . 5.48E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:17    LR . 5.45E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 5.43E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 5.40E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:50    LR . 5.37E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:21    LR . 5.34E-06\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 11 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 5.29E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 5.26E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:34    LR . 5.23E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 5.21E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 5.18E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 5.15E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 5.12E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:10    LR . 5.09E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 5.06E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 5.04E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 5.01E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:14    LR . 4.98E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:45    LR . 4.95E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:17    LR . 4.92E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 4.89E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 4.87E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:50    LR . 4.84E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:21    LR . 4.81E-06\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 12 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 4.76E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 4.73E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:33    LR . 4.70E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 4.67E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 4.65E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 4.62E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 4.59E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:10    LR . 4.56E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 4.53E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 4.50E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 4.48E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:14    LR . 4.45E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:46    LR . 4.42E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:17    LR . 4.39E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 4.36E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 4.33E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:50    LR . 4.31E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:22    LR . 4.28E-06\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 13 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 4.23E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 4.20E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:34    LR . 4.17E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 4.14E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 4.11E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 4.09E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 4.06E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:10    LR . 4.03E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 4.00E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 3.97E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 3.94E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:15    LR . 3.92E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:46    LR . 3.89E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:17    LR . 3.86E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 3.83E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 3.80E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:51    LR . 3.77E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:22    LR . 3.75E-06\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 14 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 3.70E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 3.67E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:33    LR . 3.64E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 3.61E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 3.58E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 3.55E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 3.53E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:09    LR . 3.50E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:40    LR . 3.47E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 3.44E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 3.41E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:14    LR . 3.38E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:45    LR . 3.36E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:17    LR . 3.33E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 3.30E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 3.27E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:50    LR . 3.24E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:21    LR . 3.21E-06\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 15 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 3.16E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 3.13E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:33    LR . 3.11E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 3.08E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 3.05E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 3.02E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 2.99E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:10    LR . 2.97E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 2.94E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 2.91E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 2.88E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:14    LR . 2.85E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:46    LR . 2.82E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:17    LR . 2.80E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 2.77E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 2.74E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:50    LR . 2.71E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:22    LR . 2.68E-06\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 16 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 2.63E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 2.60E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:33    LR . 2.57E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 2.55E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 2.52E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 2.49E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 2.46E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:09    LR . 2.43E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 2.40E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 2.38E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 2.35E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:14    LR . 2.32E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:45    LR . 2.29E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:16    LR . 2.26E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 2.24E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 2.21E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:50    LR . 2.18E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:21    LR . 2.15E-06\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 17 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 2.10E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 2.07E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:34    LR . 2.04E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 2.01E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 1.99E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 1.96E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 1.93E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:09    LR . 1.90E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 1.87E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 1.84E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 1.82E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:14    LR . 1.79E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:45    LR . 1.76E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:16    LR . 1.73E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 1.70E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 1.67E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:50    LR . 1.65E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:21    LR . 1.62E-06\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 18 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 1.57E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 1.54E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:33    LR . 1.51E-06\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 1.48E-06\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 1.45E-06\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 1.43E-06\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 1.40E-06\n",
            "  Batch   320  of    752.    Elapsed: 0:04:09    LR . 1.37E-06\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 1.34E-06\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 1.31E-06\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 1.28E-06\n",
            "  Batch   480  of    752.    Elapsed: 0:06:14    LR . 1.26E-06\n",
            "  Batch   520  of    752.    Elapsed: 0:06:45    LR . 1.23E-06\n",
            "  Batch   560  of    752.    Elapsed: 0:07:16    LR . 1.20E-06\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 1.17E-06\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 1.14E-06\n",
            "  Batch   680  of    752.    Elapsed: 0:08:50    LR . 1.11E-06\n",
            "  Batch   720  of    752.    Elapsed: 0:09:21    LR . 1.09E-06\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 19 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 1.04E-06\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 1.01E-06\n",
            "  Batch   120  of    752.    Elapsed: 0:01:33    LR . 9.79E-07\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 9.51E-07\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 9.22E-07\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 8.94E-07\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 8.66E-07\n",
            "  Batch   320  of    752.    Elapsed: 0:04:09    LR . 8.37E-07\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 8.09E-07\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 7.81E-07\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 7.53E-07\n",
            "  Batch   480  of    752.    Elapsed: 0:06:14    LR . 7.24E-07\n",
            "  Batch   520  of    752.    Elapsed: 0:06:45    LR . 6.96E-07\n",
            "  Batch   560  of    752.    Elapsed: 0:07:16    LR . 6.68E-07\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 6.39E-07\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 6.11E-07\n",
            "  Batch   680  of    752.    Elapsed: 0:08:50    LR . 5.83E-07\n",
            "  Batch   720  of    752.    Elapsed: 0:09:21    LR . 5.55E-07\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 20 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of    752.    Elapsed: 0:00:31    LR . 5.04E-07\n",
            "  Batch    80  of    752.    Elapsed: 0:01:02    LR . 4.75E-07\n",
            "  Batch   120  of    752.    Elapsed: 0:01:34    LR . 4.47E-07\n",
            "  Batch   160  of    752.    Elapsed: 0:02:05    LR . 4.19E-07\n",
            "  Batch   200  of    752.    Elapsed: 0:02:36    LR . 3.90E-07\n",
            "  Batch   240  of    752.    Elapsed: 0:03:07    LR . 3.62E-07\n",
            "  Batch   280  of    752.    Elapsed: 0:03:38    LR . 3.34E-07\n",
            "  Batch   320  of    752.    Elapsed: 0:04:09    LR . 3.06E-07\n",
            "  Batch   360  of    752.    Elapsed: 0:04:41    LR . 2.77E-07\n",
            "  Batch   400  of    752.    Elapsed: 0:05:12    LR . 2.49E-07\n",
            "  Batch   440  of    752.    Elapsed: 0:05:43    LR . 2.21E-07\n",
            "  Batch   480  of    752.    Elapsed: 0:06:14    LR . 1.92E-07\n",
            "  Batch   520  of    752.    Elapsed: 0:06:45    LR . 1.64E-07\n",
            "  Batch   560  of    752.    Elapsed: 0:07:17    LR . 1.36E-07\n",
            "  Batch   600  of    752.    Elapsed: 0:07:48    LR . 1.08E-07\n",
            "  Batch   640  of    752.    Elapsed: 0:08:19    LR . 7.92E-08\n",
            "  Batch   680  of    752.    Elapsed: 0:08:50    LR . 5.09E-08\n",
            "  Batch   720  of    752.    Elapsed: 0:09:21    LR . 2.26E-08\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:09:46\n",
            "\n",
            "Running Validation...\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQJzytcBDHEy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b78f950-b620-43e9-efd0-20452cb10b27"
      },
      "source": [
        "pred"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
              " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              " [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
              " [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0],\n",
              " [1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
              " [0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0],\n",
              " [0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1],\n",
              " [0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n",
              " [0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
              " [0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0],\n",
              " [0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1],\n",
              " [0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1],\n",
              " [1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
              " [1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1],\n",
              " [1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1],\n",
              " [1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0],\n",
              " [0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1],\n",
              " [1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
              " [1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1],\n",
              " [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1],\n",
              " [1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0],\n",
              " [1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1],\n",
              " [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1],\n",
              " [1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              " [0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0],\n",
              " [0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1],\n",
              " [0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              " [1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
              " [0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
              " [0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n",
              " [1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1],\n",
              " [1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1],\n",
              " [0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1],\n",
              " [0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0],\n",
              " [1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1],\n",
              " [1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
              " [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0],\n",
              " [1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1],\n",
              " [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0],\n",
              " [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1],\n",
              " [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0],\n",
              " [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
              " [0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1],\n",
              " [0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1],\n",
              " [1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0],\n",
              " [0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1],\n",
              " [1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0],\n",
              " [1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0],\n",
              " [1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1],\n",
              " [0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1],\n",
              " [0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1],\n",
              " [0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0],\n",
              " [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
              " [1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0],\n",
              " [0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1],\n",
              " [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1],\n",
              " [0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
              " [0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1],\n",
              " [0, 1, 1, 1, 1, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN7uWEM3DRKS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "93854458-3bd5-495f-8240-b3f40630ce8e"
      },
      "source": [
        "import itertools\n",
        "\n",
        "preds = list(itertools.chain(*pred))\n",
        "preds"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd5V6t6kHNpi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "09ed5a0f-b2ac-4bf7-dede-2fe05c7969e7"
      },
      "source": [
        "df_test['prediction'] = 0\n",
        "df_test"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sent0</th>\n",
              "      <th>sent1</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1175</td>\n",
              "      <td>He loves to stroll at the park with his bed</td>\n",
              "      <td>He loves to stroll at the park with his dog.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>452</td>\n",
              "      <td>The inverter was able to power the continent.</td>\n",
              "      <td>The inverter was able to power the house</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>275</td>\n",
              "      <td>The chef put extra lemons on the pizza.</td>\n",
              "      <td>The chef put extra mushrooms on the pizza.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>869</td>\n",
              "      <td>sugar is used to make coffee sour</td>\n",
              "      <td>sugar is used to make coffee sweet</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "      <td>There are beautiful flowers here and there in ...</td>\n",
              "      <td>There are beautiful planes here and there in t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>1114</td>\n",
              "      <td>If it had rained, you would got wet.</td>\n",
              "      <td>If it is a sunny day, you would got wet.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>8</td>\n",
              "      <td>ice hockey is a sport</td>\n",
              "      <td>ice hockey is a financial institution</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>1945</td>\n",
              "      <td>He put water without a container in the freeze...</td>\n",
              "      <td>He put a watermelon in the freezer for 24 hours</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>1053</td>\n",
              "      <td>The desert has sand that you can drink.</td>\n",
              "      <td>The desert is very dry, so bring water when yo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1123</td>\n",
              "      <td>My friend runs for 2 inches every day.</td>\n",
              "      <td>My friend runs for 2 miles every day.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ... prediction\n",
              "0    1175  ...          0\n",
              "1     452  ...          0\n",
              "2     275  ...          0\n",
              "3     869  ...          0\n",
              "4      50  ...          0\n",
              "..    ...  ...        ...\n",
              "995  1114  ...          0\n",
              "996     8  ...          0\n",
              "997  1945  ...          0\n",
              "998  1053  ...          0\n",
              "999  1123  ...          0\n",
              "\n",
              "[1000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlhrT6yzEWAa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "outputId": "a8522838-0289-4885-f85f-24ce4f80eafa"
      },
      "source": [
        "for i in range(len(df_test)):\n",
        "  df_test['prediction'][i] = preds [i]\n",
        "\n",
        "df_test"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sent0</th>\n",
              "      <th>sent1</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1175</td>\n",
              "      <td>He loves to stroll at the park with his bed</td>\n",
              "      <td>He loves to stroll at the park with his dog.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>452</td>\n",
              "      <td>The inverter was able to power the continent.</td>\n",
              "      <td>The inverter was able to power the house</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>275</td>\n",
              "      <td>The chef put extra lemons on the pizza.</td>\n",
              "      <td>The chef put extra mushrooms on the pizza.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>869</td>\n",
              "      <td>sugar is used to make coffee sour</td>\n",
              "      <td>sugar is used to make coffee sweet</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "      <td>There are beautiful flowers here and there in ...</td>\n",
              "      <td>There are beautiful planes here and there in t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>1114</td>\n",
              "      <td>If it had rained, you would got wet.</td>\n",
              "      <td>If it is a sunny day, you would got wet.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>8</td>\n",
              "      <td>ice hockey is a sport</td>\n",
              "      <td>ice hockey is a financial institution</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>1945</td>\n",
              "      <td>He put water without a container in the freeze...</td>\n",
              "      <td>He put a watermelon in the freezer for 24 hours</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>1053</td>\n",
              "      <td>The desert has sand that you can drink.</td>\n",
              "      <td>The desert is very dry, so bring water when yo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1123</td>\n",
              "      <td>My friend runs for 2 inches every day.</td>\n",
              "      <td>My friend runs for 2 miles every day.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ... prediction\n",
              "0    1175  ...          0\n",
              "1     452  ...          0\n",
              "2     275  ...          0\n",
              "3     869  ...          0\n",
              "4      50  ...          1\n",
              "..    ...  ...        ...\n",
              "995  1114  ...          1\n",
              "996     8  ...          1\n",
              "997  1945  ...          0\n",
              "998  1053  ...          0\n",
              "999  1123  ...          0\n",
              "\n",
              "[1000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9j2GHQqH6z2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "374b0d05-cd2a-4160-ace3-b38ba8c0af09"
      },
      "source": [
        "df_test = df_test[['id' , 'prediction']]\n",
        "df_test.to_csv('df_test.csv' , index=False, header=False)\n",
        "df_test"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1175</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>452</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>275</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>869</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>1114</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>1945</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>1053</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1123</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  prediction\n",
              "0    1175           0\n",
              "1     452           0\n",
              "2     275           0\n",
              "3     869           0\n",
              "4      50           1\n",
              "..    ...         ...\n",
              "995  1114           1\n",
              "996     8           1\n",
              "997  1945           0\n",
              "998  1053           0\n",
              "999  1123           0\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JblrZZZx2Zog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"df_test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-G03mmwH3aI",
        "colab_type": "text"
      },
      "source": [
        "##Let's take a look at our training loss over all batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68xreA9JAmG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyubuJSOzg3",
        "colab_type": "text"
      },
      "source": [
        "# 5. Performance On Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DosV94BYIYxg",
        "colab_type": "text"
      },
      "source": [
        "Now we'll load the holdout dataset and prepare inputs just as we did with the training set. Then we'll evaluate predictions using [Matthew's correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) because this is the metric used by the wider NLP community to evaluate performance on CoLA. With this metric, +1 is the best score, and -1 is the worst score. This way, we can see how well we perform against the state of the art models for this specific task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg42jJqqM68F",
        "colab_type": "text"
      },
      "source": [
        "### 5.1. Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWe0_JW21MyV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We'll need to apply all of the same steps that we did for the training data to prepare our test data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAN0LZBOOPVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df_data = pd.read_csv('taskA_trial_data.csv', error_bad_lines=False)\n",
        "df_answers = pd.read_csv('taskA_trial_answer.csv', error_bad_lines=False, header=None, names=['id', 'label'])\n",
        "df = df_data.set_index('id').join(df_answers.set_index('id'))\n",
        "\n",
        "df_appended = np.concatenate([df['sent0'] , df['sent1']])\n",
        "df = pd.DataFrame(df_appended, columns=['sentences'], )\n",
        "df.index.name = 'id'\n",
        "\n",
        "df_fliped = df_answers.replace({0:1, 1:0})\n",
        "df_Concat_answers = np.concatenate([df_fliped['label'] , df_answers['label']])\n",
        "df_Concat_answers = pd.DataFrame(df_Concat_answers , columns=['label'])\n",
        "df_Concat_answers.index.name = 'id'\n",
        "df = df_Concat_answers.merge(df , on='id')\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if df['sentences'][i][-1] != '.':\n",
        "      df['sentences'][i] = df['sentences'][i] +'.'\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentences.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16lctEOyNFik",
        "colab_type": "text"
      },
      "source": [
        "## 5.2. Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhR99IISNMg9",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hba10sXR7Xi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5jscIM8R4Gv",
        "colab_type": "text"
      },
      "source": [
        "Accuracy on the CoLA benchmark is measured using the \"[Matthews correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)\" (MCC).\n",
        "\n",
        "We use MCC here because the classes are imbalanced:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWcy0X1hirdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRaZQ4XC7kLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUM0UA1qJaVB",
        "colab_type": "text"
      },
      "source": [
        "The final score will be based on the entire test set, but let's take a look at the scores on the individual batches to get a sense of the variability in the metric between batches. \n",
        "\n",
        "Each batch has 32 sentences in it, except the last batch which has only (516 % 32) = 4 test sentences in it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xytAr_C48wnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCYZa1lQ8Jn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXx0jPc4HUfZ",
        "colab_type": "text"
      },
      "source": [
        "Cool! In about half an hour and without doing any hyperparameter tuning (adjusting the learning rate, epochs, batch size, ADAM properties, etc.) we are able to get a good score. I should also mention we didn't train on the entire training dataset, but set aside a portion of it as our validation set for legibililty of code.\n",
        "\n",
        "The library documents the expected accuracy for this benchmark [here](https://huggingface.co/transformers/examples.html#glue).\n",
        "\n",
        "You can also look at the official leaderboard [here](https://gluebenchmark.com/leaderboard/submission/zlssuBTm5XRs0aSKbFYGVIVdvbj1/-LhijX9VVmvJcvzKymxy). \n",
        "\n",
        "Note that (due to the small dataset size?) the accuracy can vary significantly with different random seeds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfjYoa6WmkN6",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlQG7qgkmf4n",
        "colab_type": "text"
      },
      "source": [
        "This post demonstrates that with a pre-trained BERT model you can quickly and effectively create a high quality model with minimal effort and training time using the pytorch interface, regardless of the specific NLP task you are interested in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUmsUOIv8EUO",
        "colab_type": "text"
      },
      "source": [
        "# Appendix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2079Qyn8Mt8",
        "colab_type": "text"
      },
      "source": [
        "## A1. Saving & Loading Fine-Tuned Model\n",
        "\n",
        "This first cell (taken from `run_glue.py` [here](https://github.com/huggingface/transformers/blob/35ff345fc9df9e777b27903f11fa213e4052595b/examples/run_glue.py#L495)) writes the model and tokenizer out to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ulTWaOr8QNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-tjHkR7lc1I",
        "colab_type": "text"
      },
      "source": [
        "Let's check out the file sizes, out of curiosity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqMzI3VTCZo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -l --block-size=K ./model_save/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr_bt2rFlgDn",
        "colab_type": "text"
      },
      "source": [
        "The largest file is the model weights, at around 418 megabytes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WUFUIQ8Cu8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -l --block-size=M ./model_save/pytorch_model.bin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzGKvOFAll_e",
        "colab_type": "text"
      },
      "source": [
        "To save your model across Colab Notebook sessions, download it to your local machine, or ideally copy it to your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trr-A-POC18_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive to this Notebook instance.\n",
        "from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxlZsafTC-V5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy the model files to a directory in your Google Drive.\n",
        "!cp -r ./model_save/ \"./drive/Shared drives/ChrisMcCormick.AI/Blog Posts/BERT Fine-Tuning/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0vstijw85SZ",
        "colab_type": "text"
      },
      "source": [
        "The following functions will load the model back from disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nskPzUM084zL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "model = model_class.from_pretrained(output_dir)\n",
        "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIWouvDrGVAi",
        "colab_type": "text"
      },
      "source": [
        "## A.2. Weight Decay\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f123ZAlF1OyW",
        "colab_type": "text"
      },
      "source": [
        "The huggingface example includes the following code block for enabling weight decay, but the default decay rate is \"0.0\", so I moved this to the appendix.\n",
        "\n",
        "This block essentially tells the optimizer to not apply weight decay to the bias terms (e.g., $ b $ in the equation $ y = Wx + b $ ). Weight decay is a form of regularization--after calculating the gradients, we multiply them by, e.g., 0.99."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxSMw0FrptiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code is taken from:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102\n",
        "\n",
        "# Don't apply weight decay to any parameters whose names include these tokens.\n",
        "# (Here, the BERT doesn't have `gamma` or `beta` parameters, only `bias` terms)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "# Separate the `weight` parameters from the `bias` parameters. \n",
        "# - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01. \n",
        "# - For the `bias` parameters, the 'weight_decay_rate' is 0.0. \n",
        "optimizer_grouped_parameters = [\n",
        "    # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.1},\n",
        "    \n",
        "    # Filter for parameters which *do* include those.\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "# Note - `optimizer_grouped_parameters` only includes the parameter values, not \n",
        "# the names."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}