# -*- coding: utf-8 -*-
"""RoBERTa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U8nPsV33tkj_GfE-uDufnk6m3O3pI4dP

#Importing files from Google Drive
"""

!pip install PyDrive

from google.colab import drive
drive.mount('/content/drive')

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

downloaded = drive.CreateFile({'id':"16_ZNThSbAQyY1Hw6CXBm6hteZ0eeLJpI"})   # replace the id with id of file you want to access
downloaded.GetContentFile('pytorch_model.bin')        # replace the file name with your file
downloaded = drive.CreateFile({'id':"1ill09R5sdg7GzBCfKfGty8eW5F9YZ-m1"})   # replace the id with id of file you want to access
downloaded.GetContentFile('config.json')

"""#Code started"""

!pip install transformers==2.4.1 -q

import torch
from transformers import *

model_name = 'roberta-large' #uncased should have do_lower_case=True
model = AutoModelForSequenceClassification.from_pretrained('./')
tokenizer = RobertaTokenizer.from_pretrained(model_name, do_lower_case=False)

model.classifier

models_names = [ 
'bert-base-uncased',
'bert-large-uncased',
'bert-base-cased',
'bert-large-cased',
'bert-base-multilingual-uncased',
'bert-base-multilingual-cased',
'bert-large-uncased-whole-word-masking',
'bert-large-cased-whole-word-masking',
'bert-large-uncased-whole-word-masking-finetuned-squad',
'bert-large-cased-whole-word-masking-finetuned-squad',
'bert-base-cased-finetuned-mrpc',
'albert-base-v1',
'albert-large-v1',
'albert-xlarge-v1',
'albert-xxlarge-v1',
'albert-base-v2',
'albert-large-v2',
'albert-xlarge-v2',
'albert-xxlarge-v2',
'xlm-mlm-en-2048',
'xlm-mlm-ende-1024',
'xlm-mlm-enfr-1024',
'xlm-mlm-enro-1024',
'xlm-mlm-xnli15-1024',
'xlm-mlm-tlm-xnli15-1024',
'xlm-clm-enfr-1024',
'xlm-clm-ende-1024',
'xlm-mlm-17-1280',
'xlm-mlm-100-1280',
'xlm-roberta-base',
'xlm-roberta-large',
'roberta-base',
'roberta-large',
'roberta-large-mnli',
'roberta-base-openai-detector',
'roberta-large-openai-detector',
'xlnet-base-cased',
'xlnet-large-cased',
]

"""# RoBERTa Fine-Tuning Tutorial with PyTorch

# 1. Setup

## 1.1. Using Colab GPU for Training
"""

import tensorflow as tf

# Get the GPU device name.
device_name = tf.test.gpu_device_name()

# The device name should look like the following:
if device_name == '/device:GPU:0':
    print('Found GPU at: {}'.format(device_name))
else:
    raise SystemError('GPU device not found')

import torch

# If there's a GPU available...
if torch.cuda.is_available():    

    # Tell PyTorch to use the GPU.    
    device = torch.device("cuda")

    print('There are %d GPU(s) available.' % torch.cuda.device_count())

    print('We will use the GPU:', torch.cuda.get_device_name(0))

# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

"""## 1.2. Installing the Hugging Face Library

# 2. Loading SemEval Dataset

## 2.1. Download & Extract
"""

!pip install wget

import wget
import os

print('Downloading dataset...')

# The URL for the dataset zip file.
!wget -q https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_data_all.csv
!wget -q https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_answers_all.csv

#trial
!wget -q https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Trial%20Data/taskA_trial_data.csv
!wget -q https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Trial%20Data/taskA_trial_answer.csv
#dev
!wget https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Dev%20Data/subtaskA_gold_answers.csv
!wget https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Dev%20Data/subtaskA_dev_data.csv

#test
!wget https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Test%20Data/subtaskA_test_data.csv

"""## 2.2. Parse

##training set
"""

import pandas as pd
import numpy as np

# Load the dataset into a pandas dataframe.
df_data = pd.read_csv('subtaskA_data_all.csv', error_bad_lines=False)
df_answers = pd.read_csv('subtaskA_answers_all.csv', error_bad_lines=False, header=None, names=['id', 'label'])

df = df_data.set_index('id').join(df_answers.set_index('id'))
df.reset_index(drop=True, inplace=True)

for i in range(len(df)):
    if df['sent0'][i][-1] != '.' or df['sent1'][i][-1] !='.' :
      df['sent0'][i] = df['sent0'][i] +'.'
      df['sent1'][i] = df['sent1'][i] +'.'
      
# Report the number of sentences.
print('Number of training sentences: {:,}\n'.format(df.shape[0]))

# Display 10 random rows from the data.
df.head(10)
df

"""##trial set"""

import pandas as pd
import numpy as np

# Load the dataset into a pandas dataframe.
df_data = pd.read_csv('taskA_trial_data.csv', error_bad_lines=False)
df_answers = pd.read_csv('taskA_trial_answer.csv', error_bad_lines=False, header=None, names=['id', 'label'])

df_trial = df_data.set_index('id').join(df_answers.set_index('id'))
df_trial.reset_index(drop=True, inplace=True)

for i in range(len(df_trial)):
    if df_trial['sent0'][i][-1] != '.' or df_trial['sent1'][i][-1] !='.' :
      df_trial['sent0'][i] = df_trial['sent0'][i] +'.'
      df_trial['sent1'][i] = df_trial['sent1'][i] +'.'
      
# Report the number of sentences.
print('Number of training sentences: {:,}\n'.format(df_trial.shape[0]))

# Display 10 random rows from the data.
df_trial.head(10)
df_trial

"""##training set + trial set"""

frames = [df, df_trial]
df = pd.concat(frames, ignore_index=True)
df

"""##dev set"""

import pandas as pd
import numpy as np

# Load the dataset into a pandas dataframe.
df_data = pd.read_csv('subtaskA_dev_data.csv', error_bad_lines=False)
df_answers = pd.read_csv('subtaskA_gold_answers.csv', error_bad_lines=False, header=None, names=['id', 'label'])

df_dev = df_data.set_index('id').join(df_answers.set_index('id'))
df_dev.reset_index(drop=True, inplace=True)

for i in range(len(df_dev)):
    if df_dev['sent0'][i][-1] != '.' or df_dev['sent1'][i][-1] !='.' :
      df_dev['sent0'][i] = df_dev['sent0'][i] +'.'
      df_dev['sent1'][i] = df_dev['sent1'][i] +'.'
      
# Report the number of sentences.
print('Number of training sentences: {:,}\n'.format(df_dev.shape[0]))

# Display 10 random rows from the data.
df_dev.head(10)

"""##test set"""

import pandas as pd
import numpy as np

# Load the dataset into a pandas dataframe.
df_test = pd.read_csv('subtaskA_test_data.csv', error_bad_lines=False)
df_test

"""#Let's extract the sentences and labels of our training set as numpy ndarrays."""

# Get the lists of sentences and their labels.
sentence1 = df.sent1.values
sentence0 = df.sent0.values
labels = df.label.values



#test data
#sentence1 = df_test.sent1.values
#sentence0 = df_test.sent0.values
#test_labels = df_test.label.values


#dev
sentence1 = df_dev.sent1.values
sentence0 = df_dev.sent0.values
dev_labels = df_dev.label.values

dev_labels

"""# 3. Tokenization & Input Formatting

## 3.2. Required Formatting

## 3.2. Sentences to IDs using tokenizer.encode()
"""

MAX_LEN = 64

"""##encode_plus for training + trial data"""

# Tokenize all of the sentences and map the tokens to thier word IDs.
input_ids = []
input_type_ids = []
attention_masks = []

# For every sentence...
for i in range(len(df)):
    tokenizer.padding_side = 'right'
    encoded_sent = tokenizer.encode_plus(
                        df['sent0'][i],            # Sentence to encode.
                        df['sent1'][i],
                        add_special_tokens = True, 
                        max_length = MAX_LEN,
                        pad_to_max_length = True
                        )

    #Add the encoded sentence to the list.
    input_ids.append(encoded_sent['input_ids'])
    input_type_ids.append(encoded_sent['token_type_ids'])
    attention_masks.append(encoded_sent['attention_mask'])

#Print last sentences of df, now as a list of IDs.
print('original: ' , df['sent0'][0], df['sent1'][0])
print('Token IDs:', input_ids[10])
print('Type IDs:', input_type_ids[10])
print('MASK IDs:', attention_masks[10])

#Original:  I have a desk on my lamp.
#Tokenized:  ['▁', 'I', '▁have', '▁a', '▁desk', '▁on', '▁my', '▁lamp', '.']
#Token IDs:  [13, 1, 57, 21, 2911, 27, 51, 6792, 9]
tokenizer.cls_token_id, tokenizer.sep_token_id

"""##encode_plus for test data"""

# Tokenize all of the sentences and map the tokens to thier word IDs.
validation_inputs = []
validation_type_ids = []
validation_masks = []

# For every sentence...
for i in range(len(df_test)):
    tokenizer.padding_side = 'right'
    encoded_sent = tokenizer.encode_plus(
                        df_test['sent0'][i],            # Sentence to encode.
                        df_test['sent1'][i],
                        add_special_tokens = True, 
                        max_length = MAX_LEN,
                        pad_to_max_length = True
                        )

    #Add the encoded sentence to the list.
    validation_inputs.append(encoded_sent['input_ids'])
    validation_type_ids.append(encoded_sent['token_type_ids'])
    validation_masks.append(encoded_sent['attention_mask'])

#Print last sentences of df, now as a list of IDs.
print('original: ' , df_test['sent0'][0], df_test['sent1'][0])
print('Token IDs:', validation_inputs[10])
print('Type IDs:', validation_type_ids[10])
print('MASK IDs:', validation_masks[10])

#Original:  I have a desk on my lamp.
#Tokenized:  ['▁', 'I', '▁have', '▁a', '▁desk', '▁on', '▁my', '▁lamp', '.']
#Token IDs:  [13, 1, 57, 21, 2911, 27, 51, 6792, 9]
tokenizer.cls_token_id, tokenizer.sep_token_id

"""## 3.5. Training & Validation Split

##The following code split 10 % of training set as validation **set**
"""

# Use train_test_split to split our data into train and validation sets for training
from sklearn.model_selection import train_test_split
import random

# Set the seed value all over the place to make this reproducible.
seed_val = 42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

# Use 90% for training and 10% for validation.
train_inputs, validation_inputs, train_labels, validation_labels, train_type_ids, validation_type_ids, train_masks, validation_masks = (
    train_test_split(input_ids, labels, input_type_ids, attention_masks, random_state=2018, test_size=0.1))

"""##The following set has all training data for training set and use trial data for validation set"""

# Use train_test_split to split our data into train and validation sets for training
from sklearn.model_selection import train_test_split
import random

# Set the seed value all over the place to make this reproducible.
seed_val = 42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

train_inputs, train_labels, train_type_ids, train_masks = input_ids, labels , input_type_ids , attention_masks


validation_inputs,  validation_type_ids, validation_masks = validation_inputs, validation_type_ids, validation_masks

"""## 3.6. Converting to PyTorch Data Types"""

# Convert all inputs and labels into torch tensors, the required datatype 
# for our model.

train_type_ids = torch.tensor(train_type_ids) 
validation_type_ids = torch.tensor(validation_type_ids)

train_inputs = torch.tensor(train_inputs)
validation_inputs = torch.tensor(validation_inputs)

train_labels = torch.tensor(train_labels)
#validation_labels = torch.tensor(validation_labels)

train_masks = torch.tensor(train_masks)
validation_masks = torch.tensor(validation_masks)

from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

# The DataLoader needs to know our batch size for training, so we specify it 
# here.
# For fine-tuning BERT on a specific task, the authors recommend a batch size of
# 16 or 32.

batch_size = 16

# Create the DataLoader for our training set.
train_data = TensorDataset(train_inputs, train_masks, train_labels, train_type_ids)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

# Create the DataLoader for our validation set.
validation_data = TensorDataset(validation_inputs, validation_masks, validation_type_ids)
validation_sampler = SequentialSampler(validation_data)
validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)

"""# 4. Train Our Classification Model

## 4.2. Optimizer & Learning Rate Scheduler
"""

model.cuda();
# Note: AdamW is a class from the huggingface library (as opposed to pytorch) 
# I believe the 'W' stands for 'Weight Decay fix"
optimizer = AdamW(model.parameters(),
                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5
                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.
                )

from transformers import get_linear_schedule_with_warmup

# Number of training epochs (authors recommend between 2 and 4)
epochs = 20

# Total number of training steps is number of batches * number of epochs.
print('len(train_dataloader)', len(train_dataloader))
total_steps = len(train_dataloader) * epochs
print('total_steps', total_steps)
warmup_steps = int(0.06 * total_steps)
print('warmup_steps', warmup_steps)
# Create the learning rate scheduler.
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = warmup_steps, # Default value in run_glue.py
                                            num_training_steps = total_steps)

"""## 4.3. Training Loop

###Define a helper function for calculating accuracy.
"""

import numpy as np

# Function to calculate the accuracy of our predictions vs labels
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

import time
import datetime

def format_time(elapsed):
    '''
    Takes a time in seconds and returns a string hh:mm:ss
    '''
    # Round to the nearest second.
    elapsed_rounded = int(round((elapsed)))
    
    # Format as hh:mm:ss
    return str(datetime.timedelta(seconds=elapsed_rounded))

"""We're ready to kick off the training!"""

import random

# This training code is based on the `run_glue.py` script here:
# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128


# Set the seed value all over the place to make this reproducible.
seed_val = 42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

# Store the average loss after each epoch so we can plot them.
loss_values = []

# For each epoch...
for epoch_i in range(0, epochs):
    # ========================================
    #               Training
    # ========================================
    
    # Perform one full pass over the training set.

    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    print('Training...')

    # Measure how long the training epoch takes.
    t0 = time.time()

    # Reset the total loss for this epoch.
    total_loss = 0

    # Put the model into training mode. Don't be mislead--the call to 
    # `train` just changes the *mode*, it doesn't *perform* the training.
    # `dropout` and `batchnorm` layers behave differently during training
    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)
    model.train()

    # For each batch of training data...
    for step, batch in enumerate(train_dataloader):

        # Progress update every 40 batches.
        if step % 40 == 0 and not step == 0:
            # Calculate elapsed time in minutes.
            elapsed = format_time(time.time() - t0)
            
            # Report progress.
            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}    LR . {:.2E}'.format(step, len(train_dataloader), elapsed, scheduler.get_lr()[0]))

        # Unpack this training batch from our dataloader. 
        #
        # As we unpack the batch, we'll also copy each tensor to the GPU using the 
        # `to` method.
        #
        # `batch` contains three pytorch tensors:
        #   [0]: input ids 
        #   [1]: attention masks
        #   [2]: labels 
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)
        b_type_ids = batch[3].to(device)
        # Always clear any previously calculated gradients before performing a
        # backward pass. PyTorch doesn't do this automatically because 
        # accumulating the gradients is "convenient while training RNNs". 
        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)
        model.zero_grad()        

        # Perform a forward pass (evaluate the model on this training batch).
        # This will return the loss (rather than the model output) because we
        # have provided the `labels`.
        # The documentation for this `model` function is here: 
        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification
        outputs = model(b_input_ids, 
                    token_type_ids=b_type_ids, 
                    attention_mask=b_input_mask, 
                    labels=b_labels)
        
        # The call to `model` always returns a tuple, so we need to pull the 
        # loss value out of the tuple.
        loss = outputs[0]

        # Accumulate the training loss over all of the batches so that we can
        # calculate the average loss at the end. `loss` is a Tensor containing a
        # single value; the `.item()` function just returns the Python value 
        # from the tensor.
        total_loss += loss.item()

        # Perform a backward pass to calculate the gradients.
        loss.backward()

        # Clip the norm of the gradients to 1.0.
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # Update parameters and take a step using the computed gradient.
        # The optimizer dictates the "update rule"--how the parameters are
        # modified based on their gradients, the learning rate, etc.
        optimizer.step()

        # Update the learning rate.
        scheduler.step()

    # Calculate the average loss over the training data.
    avg_train_loss = total_loss / len(train_dataloader)            
    
    # Store the loss value for plotting the learning curve.
    loss_values.append(avg_train_loss)

    print("")
    print("  Average training loss: {0:.2f}".format(avg_train_loss))
    print("  Training epcoh took: {:}".format(format_time(time.time() - t0)))
        
    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    print("")
    print("Running Validation...")

    t0 = time.time()

    # Put the model in evaluation mode--the dropout layers behave differently
    # during evaluation.
    model.eval()

    # Tracking variables 
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    pred = []

    # Evaluate data for one epoch
    for batch in validation_dataloader:
        
        # Add batch to GPU
        batch = tuple(t.to(device) for t in batch)
        
        # Unpack the inputs from our dataloader
        b_input_ids, b_input_mask, b_type_ids = batch

        # Telling the model not to compute or store gradients, saving memory and
        # speeding up validation
        with torch.no_grad():        

            # Forward pass, calculate logit predictions.
            # This will return the logits rather than the loss because we have
            # not provided labels.
            # token_type_ids is the same as the "segment ids", which 
            # differentiates sentence 1 and 2 in 2-sentence tasks.
            # The documentation for this `model` function is here: 
            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification
            outputs = model(b_input_ids, 
                            token_type_ids=b_type_ids, 
                            attention_mask=b_input_mask)
        
        # Get the "logits" output by the model. The "logits" are the output
        # values prior to applying an activation function like the softmax.
        logits = outputs[0]

        # Move logits and labels to CPU
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        
        # Calculate the accuracy for this batch of test sentences.
        #tmp_eval_accuracy = flat_accuracy(logits, label_ids)
        pred.append(list(np.argmax(logits, axis=1).flatten()))
        
        # Accumulate the total accuracy.
        #eval_accuracy += tmp_eval_accuracy

        # Track the number of batches
        nb_eval_steps += 1

    # Report the final accuracy for this validation run.
    #print("  Accuracy: {0:.2f}".format(eval_accuracy/nb_eval_steps))
    print("  Validation took: {:}".format(format_time(time.time() - t0)))

print("")
print("Training complete!")

pred

import itertools

preds = list(itertools.chain(*pred))
preds

df_test['prediction'] = 0
df_test

for i in range(len(df_test)):
  df_test['prediction'][i] = preds [i]

df_test

df_test = df_test[['id' , 'prediction']]
df_test.to_csv('df_test.csv' , index=False, header=False)
df_test

from google.colab import files
files.download("df_test.csv")