# -*- coding: utf-8 -*-
"""SemEval4-First code-Seperate sentenses.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IkN29pfMKV_jPWH_9TBtX0sylkCSZYbl

This ipynb file is the first effort on SemEval4. When I preprocessed data and converted it to a text column data frame including 20000 sent0 and sent1. After that I created a databunch and then a cnn classifier and then training.

#Updating Stuff
"""

# Make ready for fast.ai
!curl -s https://course.fast.ai/setup/colab | bash
!pip -q install git+https://github.com/fastai/fastai.git -U

"""#Importing Libraries"""

import fastai
from fastai.text import *
from fastai import *
import pandas as pd
import glob
from numpy import random

"""#Downloading and preparing Traning Data

##Downloading CSV files from task4 repo
"""

subtaskA_URL = 'https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_data_all.csv'

subtaskA_Answers_URL = 'https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_answers_all.csv'

import urllib.request
urllib.request.urlretrieve(subtaskA_URL , '/content/subtaskA_data_all.csv')

urllib.request.urlretrieve(subtaskA_Answers_URL , '/content/subtaskA_answers_all.csv')

"""##Read subtaskA_data_all.csv
subtaskA_data_all is the released training set of this task
"""

df1 = pd.read_csv(subtaskA_URL , index_col=0)
df1

"""##append all sentences in one column

instead of having two columns for sent0 and sent1 we append them to one column, 10000 sent0 + 10000 sent1
"""

df_appended = np.concatenate([df1['sent0'] , df1['sent1']])
#df_appended
df_original = pd.DataFrame(df_appended, columns=['sentenses'], )
df_original.index.name = 'id'
df_original
#df_original[0:1] ,  df_original[10000:10001]

"""##Read subtaskA_answers_all.csv

The labels are given in a different csv file
"""

df2 = pd.read_csv(subtaskA_Answers_URL , header=None)
df2.columns = ['id', 'answers']
df2.set_index('id', inplace=True)
df2

"""##Create a dataframe with fliped value & concatenate to df_Concat_answers

As now we have 20000 sentences we are going to label them so we should create a dataframe with 20000 labels. the one that does not make sense is labeled by 1 and the one which does make sense is labeled by 0.

For sent0 all lables are fliped and for sent1 all lables are same.
"""

df_fliped = df2.replace({0:1, 1:0})
df_fliped
df_Concat_answers = np.concatenate([df_fliped['answers'] , df2['answers']])
df_Concat_answers
df_Concat_answers = pd.DataFrame(df_Concat_answers , columns=['answers'])
df_Concat_answers.index.name = 'id'
df_Concat_answers

"""##Dataframe 
The final dataframe/Training set
"""

df = df_Concat_answers.merge(df_original , on='id')
df

"""##If you want to use from_csv"""

#Writing a pandas DataFrame to CSV file
df.to_csv("training_all.csv")

df['sentenses'][1] , df['sentenses'][10001]

df[0:8000]

"""#Seperate training set & test set & validation set

19000 sampels for training data
1000 samples for test data
###************Make a function instead of doing it seperatly***************
"""

#training set
df_training1 = df[0:8000]
df_training2 = df[10000:18000]
#df_training1 = pd.DataFrame(df_training1)
#df_training2 = pd.DataFrame(df_training2)
df_training = pd.concat([df_training1,df_training2])
#df_training = pd.DataFrame(df_training)
df_training.columns = ['answers' , 'sentenses']
df_training.index.name = 'id'
df_training

#test set
df_test1 = df[9000:10000]
df_test2 = df[19000:20000]
#df_test1 = pd.DataFrame(df_test1)
#df_test2 = pd.DataFrame(df_test2)
df_test = pd.concat([df_test1,df_test2])
#df_test = pd.DataFrame(df_test)
df_test.columns = ['answers' , 'sentenses']
df_test.index.name = 'id'
df_test

df_test[0:1] , df_test[1000:1001]

#validation set
df_valid1 = df[8000:9000]
df_valid2 = df[18000:19000]
#df_valid1 = pd.DataFrame(df_valid1)
#df_valid2 = pd.DataFrame(df_valid2)
df_valid = pd.concat([df_valid1,df_valid2])
#df_valid = pd.DataFrame(df_valid)
df_valid.columns = ['answers' , 'sentenses']
df_valid.index.name = 'id'
df_valid

"""##If you want to use from_csv"""

df_training.to_csv("training-19000.csv")
df_test.to_csv("test-1000.csv")

"""#Tokenizing- ClsDataBunch"""

data = TextClasDataBunch.from_df('/content', train_df= df_training, valid_df= df_valid, test_df= df_test , text_cols='sentenses', label_cols = 'answers', bs=16)

data.show_batch()

data,  data.classes

#data.vocab.itos[:50]
#data.vocab.stoi
#len(data.vocab.itos)
#t1 = data.train_ds[500][0] 
#t1
#t2 = data.train_ds[500][1]
#t2

"""#Classifier"""

learn_classifier = text_classifier_learner(data , AWD_LSTM, drop_mult=0)

learn_classifier.lr_find()

learn_classifier.recorder.plot()

#learn_classifier.fit_one_cycle(1, 1e-02, moms=(0.8,0.7))

"""##Training"""

learn_classifier.fit_one_cycle(20, 1e-03, moms=(0.8,0.7))

learn_classifier.save("First")

"""###First Freezing and Training"""

learn_classifier.freeze_to(-1)

learn_classifier.fit_one_cycle(20, 1e-4, moms=(0.8,0.7))

learn_classifier.save('Second')
#learn_classifier.load('Second');

"""###Second freezing and Training"""

learn_classifier.freeze_to(-2)
learn_classifier.fit_one_cycle(40, 1e-4, moms=(0.8,0.7))

learn_classifier.load('Second');

"""###Third Freezing and training"""

learn_classifier.freeze_to(-3)
learn_classifier.fit_one_cycle(30, 1e-3, moms=(0.8,0.7))