# -*- coding: utf-8 -*-
"""ULMFiT-Normalized.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fERbXLFi4QTB_jh8EMu43sbQFF4TshoU

#Tesla
"""

!nvidia-smi | grep "|   0"

"""#Updating Stuff"""

# Make ready for fast.ai
!curl -s https://course.fast.ai/setup/colab | bash
!pip -q install git+https://github.com/fastai/fastai.git -U

"""#Importing Libraries"""

import fastai
from fastai.text import *
from fastai import *
import pandas as pd
import glob
from numpy import random

"""#Downloading and preparing Traning Data

##Downloading CSV files from task4 repo
"""

subtaskA_URL = 'https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_data_all.csv'

subtaskA_Answers_URL = 'https://raw.githubusercontent.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation/master/Training%20%20Data/subtaskA_answers_all.csv'

import urllib.request
urllib.request.urlretrieve(subtaskA_URL , '/content/subtaskA_data_all.csv')

urllib.request.urlretrieve(subtaskA_Answers_URL , '/content/subtaskA_answers_all.csv')

"""##Read subtaskA_data_all.csv
subtaskA_data_all is the released training set of this task
"""

df1 = pd.read_csv(subtaskA_URL , index_col=0)
df1

#df1['Sentences_Seperated'] = df1[['sent0', 'sent1']].apply(lambda x: ' , '.join(x), axis=1)
#df1.drop(['sent0' , 'sent1'], axis=1, inplace=True)
#df1

"""##Read subtaskA_answers_all.csv

The labels are given in a different csv file
"""

df2 = pd.read_csv(subtaskA_Answers_URL , header=None)
df2.columns = ['id', 'answers']
df2.set_index('id', inplace=True)
df2

"""##Dataframe 
The final dataframe/Training set
"""

df = df1.merge(df2 , on='id')
df['predicted_answers'] = None
#df = df.assign(predicted_answers = np.nan)
df

"""##If you want to use from_csv"""

#Writing a pandas DataFrame to CSV file
# df.to_csv("training_all.csv")

# df[0:8000]

"""#Seperate training set & test set & validation set

8000 sampels for training data , 1000 sampels for validation data, 1000 samples for test data
"""

df_training = df[0:8000]
df_valid = df[8000:9000]
df_test = df[9000:10000]
df_test = df_test.drop( 'answers' , axis = 1)
df_training

df_valid

df_test

"""##If you want to use from_csv"""

df_training.to_csv("training.csv")
df_test.to_csv("test.csv")
df_valid.to_csv("valid.csv")
df_training['sent0']

for i in range(8000):
  print(df_training['sent0'][i])

"""#Language Model

##Creating a databunch for language model
"""

data_lm_0 = TextLMDataBunch.from_df('/content', train_df= df_training, valid_df= df_valid, text_cols= 'sent0', bs=16)
data_lm_1 = TextLMDataBunch.from_df('/content', train_df= df_training, valid_df= df_valid, text_cols= 'sent1', bs=16)

data_lm_1

data_lm_0.train_ds.x

print(data_lm_0.train_ds.x[3755].text.split(' '))
print(data_lm_1.train_ds.x[3755].text.split(' '))

data_lm_0.show_batch(1) #, data_lm_1.show_batch()

"""##Creating and Training Language model using pretraind AWD_LSTM"""

learn_0 = language_model_learner(data_lm_0, AWD_LSTM, drop_mult=0.3)
learn_1 = language_model_learner(data_lm_1, AWD_LSTM, drop_mult=0.3)

#learn.lr_find()
#learn.recorder.plot(skip_end = 15)
#learn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7))

"""#Without Normalization

##TextProcessor function

This function gets the sentenses and creates a list of tuples. First element of tuples are the previous given words and second element is the target words that we're gonna predict and compute it's probabilities. This function returns the following list
 
([('', 'I'),
  (' I', 'put'),
  (' I put', 'all'),
  (' I put all', 'my'),
  (' I put all my', 'stuff'),
  (' I put all my stuff', 'in'),
  (' I put all my stuff in', 'the'),
  (' I put all my stuff in the', 'drawer'),
  (' I put all my stuff in the drawer', 'to'),
  (' I put all my stuff in the drawer to', 'travel'),
  (' I put all my stuff in the drawer to travel', '.')],
"""

def textProcessor(TEXT):
  TEXT = TEXT.replace('.', ' .')
  Text_list = TEXT.split()
  Text_list.insert(0 , '')
  goal = [] 
 
  for i in range(len(Text_list)):
    string = ' '.join(word for word in Text_list[0:i+1])
    goal.append(string)

  Text_list.remove('')
  del goal[-1]
  tupled = list(zip(goal , Text_list))
  return tupled


#print(data_lm_0.train_ds.x[3755].text)
#print(data_lm_0.train_ds.x[3755].text.split(' '))
#print(data_lm_1.train_ds.x[3755].text.split(' '))
TEXT = data_lm_0.train_ds.x[7].text
textProcessor(TEXT)
#textProcessor(data_lm_1.train_ds.x[3755].text)

"""## multiple_probblty function

it gets the list of tuples and compute the probabilities of each target element given the previous seen words(first element of tuples).
So the for loop goes over tuples and
"""

def multiple_probblty(tupled):
  target_prbblty = []
  for element in tupled:

    xb,yb = learn_0.data.one_item(element[0])
    learn_0.model.reset() # reset the model.hidden values for the next time that we want to do pred_batch.
    
    #[0] because we have one batches and [-1] because we need the output of the last layer of SWD-LSTM
    #learn.pred_batch.shape() has three dimensions [number of batches , length of the text(it can be different with the text because of the tokenization), vocab size]
    #vocab_prbblty is a list of probabilities given the elemet1 for all vocabulary of training data
    vocab_prbblty = learn_0.pred_batch(batch=(xb,yb))[0][-1]
    trgt = vocab_prbblty[learn_0.data.vocab.stoi[element[1]]].item()
    #print(element,trgt)
    
    #target_prbblty is a list of probability of all targets (target=element[1])
    target_prbblty.append(trgt)

  #multiplying probabilities
  #print(target_prbblty)
  result = reduce((lambda p, q: p * q), target_prbblty)
  #normalize based on the lentgh of sentences
  #result = pow(result,(1/len(tupled)))
  #print('Result:',result)
  return result

multiple_probblty(textProcessor(data_lm_0.train_ds.x[7].text))  
multiple_probblty(textProcessor('xxbos a xxup girl xxup won xxup the xxup race xxup with xxup her xxup horse'));

"""##Prediction"""

def df_to_model():

  for i in range(8000,9000):
    multiple_prbblty_sent0 = multiple_probblty(textProcessor(data_lm_0.valid_ds.x[i-8000].text))
    multiple_prbblty_sent1 = multiple_probblty(textProcessor(data_lm_1.valid_ds.x[i-8000].text))
    #print(multiple_prbblty_sent0 , multiple_prbblty_sent1)
    
    if multiple_prbblty_sent0 > multiple_prbblty_sent1:
      #It means sent0 makes sense so the answer is 1
      df_valid.loc[i ,'predicted_answers'] = 1      

    else: #elif multiple_prbblty_sent0 < multiple_prbblty_sent1:
      #It means sent1 makes sense so the answer is 0
      df_valid.loc[i ,'predicted_answers'] = 0

df_to_model()
df_valid.head(20)

df_valid.head(15)

"""##Accuracy"""

len(df_valid)

def Accuracy():
  accuracy_count = 0
  for i in range(8000,9000):
    if df_valid['answers'][i] == df_valid['predicted_answers'][i]:
      accuracy_count +=1
  print(accuracy_count/1000)

Accuracy()

"""#With Normalization"""

def textProcessor(TEXT):
  TEXT = TEXT.replace('.', ' .')
  Text_list = TEXT.split()
  Text_list.insert(0 , '')
  goal = [] 
 
  for i in range(len(Text_list)):
    string = ' '.join(word for word in Text_list[0:i+1])
    goal.append(string)

  Text_list.remove('')
  del goal[-1]
  tupled = list(zip(goal , Text_list))
  return tupled


#print(data_lm_0.train_ds.x[3755].text)
#print(data_lm_0.train_ds.x[3755].text.split(' '))
#print(data_lm_1.train_ds.x[3755].text.split(' '))
TEXT = data_lm_0.train_ds.x[7].text
textProcessor(TEXT)
#textProcessor(data_lm_1.train_ds.x[3755].text)

def multiple_probblty(tupled):
  target_prbblty = []
  for element in tupled:

    xb,yb = learn_0.data.one_item(element[0])
    learn_0.model.reset() # reset the model.hidden values for the next time that we want to do pred_batch.
    
    #[0] because we have one batches and [-1] because we need the output of the last layer of SWD-LSTM
    #learn.pred_batch.shape() has three dimensions [number of batches , length of the text(it can be different with the text because of the tokenization), vocab size]
    #vocab_prbblty is a list of probabilities given the elemet1 for all vocabulary of training data
    vocab_prbblty = learn_0.pred_batch(batch=(xb,yb))[0][-1]
    trgt = vocab_prbblty[learn_0.data.vocab.stoi[element[1]]].item()
    #print(element,trgt)
    
    #target_prbblty is a list of probability of all targets (target=element[1])
    target_prbblty.append(trgt)

  #multiplying probabilities
  #print(target_prbblty)
  result = reduce((lambda p, q: p * q), target_prbblty)
  #normalize based on the lentgh of sentences
  result = pow(result,(1/len(tupled)))
  #print('Result:',result)
  return result

multiple_probblty(textProcessor(data_lm_0.train_ds.x[7].text))  
multiple_probblty(textProcessor('xxbos a xxup girl xxup won xxup the xxup race xxup with xxup her xxup horse'));

def df_to_model():

  for i in range(8000,9000):
    multiple_prbblty_sent0 = multiple_probblty(textProcessor(data_lm_0.valid_ds.x[i-8000].text))
    multiple_prbblty_sent1 = multiple_probblty(textProcessor(data_lm_1.valid_ds.x[i-8000].text))
    #print(multiple_prbblty_sent0 , multiple_prbblty_sent1)
    
    if multiple_prbblty_sent0 > multiple_prbblty_sent1:
      #It means sent0 makes sense so the answer is 1
      df_valid.loc[i ,'predicted_answers'] = 1      

    else: #elif multiple_prbblty_sent0 < multiple_prbblty_sent1:
      #It means sent1 makes sense so the answer is 0
      df_valid.loc[i ,'predicted_answers'] = 0

df_to_model()
df_valid.head()

df_valid.tail()

"""##Check accuracy"""

def Accuracy():
  accuracy_count = 0
  for i in range(8000 , 9000):
    if df_valid['answers'][i] == df_valid['predicted_answers'][i]:
      accuracy_count +=1
  print(accuracy_count/1000)

Accuracy()

"""#Small example"""

def textProcessor(TEXT1 , TEXT2):

  TEXT1 = TEXT1.replace('.', ' .')
  TEXT2 = TEXT2.replace('.', ' .')
  Text_list1 = TEXT1.split()
  Text_list2 = TEXT2.split()
  Text_list1.insert(0 , '')
  Text_list2.insert(0 , '')
  goal1 = [] 
  goal2 = [] 

  for i in range(len(Text_list1)):
    string = ' '.join(word for word in Text_list1[0:i+1])
    goal1.append(string)

  for i in range(len(Text_list2)):
    string = ' '.join(word for word in Text_list2[0:i+1])
    goal2.append(string)

  Text_list1.remove('')
  Text_list2.remove('')
  del goal1[-1]
  del goal2[-1]
  tupled1 = list(zip(goal1 , Text_list1))
  tupled2 = list(zip(goal2 , Text_list2))
  tupled = [tupled1, tupled2]
  return tupled

TEXT1 = "a little baby can't feed itself."
TEXT2 = "a little baby can feed itself."
textProcessor(TEXT1 , TEXT2)
#tupled1 , tupled2

def multiple_probblty(tupled):
  target_prbblty1 = []
  target_prbblty2 = []
  
  for element in tupled[0]:
    xb,yb = learn.data.one_item(element[0])
   # print(xb , yb)
    learn.model.reset() # reset the model.hidden values for the next time that we want to do pred_batch.
    
    #[0] because we have one batches and [-1] because we need the output of the last layer of SWD-LSTM
    #learn.pred_batch.shape() has three dimensions [number of batches , length of the text(it can be different with the text because of the tokenizatio) , vocab size]
    #vocab_prbblty is a list of probabilities given the elemet1 for all vocabulary of training data
    vocab_prbblty1 = learn.pred_batch(batch=(xb,yb))[0][-1]
    #print(vocab_prbblty1)
    
    #target_prbblty is a list of probability of all targets (target=element[1])
    target_prbblty1.append(vocab_prbblty1[learn.data.vocab.stoi[element[1]]].item())
  print('xb and yb for sent0:', xb , yb)
  print('vocabulary probability:' , vocab_prbblty1)
  print('target' , target_prbblty1)
  print('------------------------------------------------------')
  
  for element in tupled[1]:
    xb,yb = learn.data.one_item(element[0])
    learn.model.reset() # reset the model.hidden values for the next time that we want to do pred_batch.
    vocab_prbblty2 = learn.pred_batch(batch=(xb,yb))[0][-1]
    #print(vocab_prbblty2)
    target_prbblty2.append(vocab_prbblty2[learn.data.vocab.stoi[element[1]]].item())
  print('xb and yb for sent1:', xb , yb)
  print('vocabulary probability:' , vocab_prbblty2)
  print('target' , target_prbblty2)

    #multiplying probabilities

  result1 = reduce((lambda p, q: p * q), target_prbblty1)
  result2 = reduce((lambda p, q: p * q), target_prbblty2)
  #normalize based on the lentgh of sentences
  result1 = pow(result1,(1/len(tupled1)))
  result2 = pow(result2,(1/len(tupled2)))
  return result1 , result2
  
multiple_probblty(textProcessor(TEXT1 , TEXT2))

def df_to_model(TEXT1 , TEXT2):
    answer = 0
  #for i in range(8000, 9000):
    multiple_prbblty_sent0 , multiple_prbblty_sent1 = multiple_probblty(textProcessor(TEXT1 , TEXT2))
    print(multiple_prbblty_sent0 , multiple_prbblty_sent1)
    
    if multiple_prbblty_sent0 > multiple_prbblty_sent1:
      answer = 1
      #It means sent0 makes sense so the answer is 1
      #df_valid.at[i ,'predicted_answers'] = 1      

    elif multiple_prbblty_sent0 < multiple_prbblty_sent1:
      answer = 0
      #It means sent1 makes sense so the answer is 0
      #df_valid.at[i ,'predicted_answers'] = 0
    return answer

df_to_model(TEXT1 , TEXT2)

"""#Manual computation. All things that are done with the previous functions can be done here manually"""

learn.get_preds()[0].shape

TEXT1 = "a little baby can't feed itself."
tupled = [('', 'a'),
  (' a', 'little'),
  (' a little', 'baby'),
  (' a little baby', "can't"),
  (" a little baby can't", 'feed'),
  (" a little baby can't feed", 'itself'),
  (" a little baby can't feed itself", '.')]
target =[]

for i in tupled:
  xb,yb = learn.data.one_item(i[0])
  learn.model.reset()
  res = learn.pred_batch(batch=(xb,yb))[0][-1]
  target.append(res[learn.data.vocab.stoi[i[1]]].item())
print(xb , yb)
print(res)
print(target)

result = reduce((lambda p, q: p * q), target)
result = pow(result,1/7)
print(result)

TEXT2 = "a little baby can feed itself."

tupled = [('', 'a'),
  (' a', 'little'),
  (' a little', 'baby'),
  (' a little baby', 'can'),
  (' a little baby can', 'feed'),
  (' a little baby can feed', 'itself'),
  (' a little baby can feed itself', '.')]

target =[]

for i in tupled:
  xb,yb = learn.data.one_item(i[0])
  learn.model.reset()
  res = learn.pred_batch(batch=(xb,yb))[0][-1]
  target.append(res[learn.data.vocab.stoi[i[1]]].item())
print(xb , yb)
print(res)
print(target)

result = reduce((lambda p, q: p * q), target)
result = pow(result,1/7)
print(result)

t1 = 0.0005794158908777242
t2 = 0.0005206210424696941
if t1>t2:
  print('1')
else:
  print('0')

"""##The probablity of make sence words and non make sense word"""

res[learn.data.vocab.stoi['a']]

res[learn.data.vocab.stoi['little']]

res[learn.data.vocab.stoi['baby']]

"""previous

#Tokenizing- ClsDataBunch

Sentences are seperated by a special tokens (mark_fields = true parameter)
"""

data = TextClasDataBunch.from_df('/content', train_df= df_training, valid_df= df_valid, test_df= df_test  , text_cols= [0 , 1], label_cols =[2], mark_fields= True, bs=16)

data.show_batch()

data,  data.classes

#data.vocab.itos[:50]
#data.vocab.stoi
#len(data.vocab.itos)
#t1 = data.train_ds[500][0] 
#t1
#t2 = data.train_ds[500][1]
#t2

"""#Classifier"""

learn_classifier = text_classifier_learner(data , AWD_LSTM, drop_mult=0)

learn_classifier.lr_find()

learn_classifier.recorder.plot()

"""##Training"""

learn_classifier.fit_one_cycle(2, 1e-04, moms=(0.8,0.7))

learn_classifier.save("First")
learn_classifier.load('First');

learn_classifier.recorder.plot_losses()

learn_classifier.model

learn_classifier.unfreeze()
learn_classifier.summary()

for index, layer in enumerate(learn_classifier.layer_groups):
  print('Layer Group Index: ', index, layer)

def summary_trainable(learner):
  result = []
  total_params_element = 0
  def check_trainable(module):
    nonlocal total_params_element
    if len(list(module.children())) == 0:
      num_param = 0
      num_trainable_param = 0
      num_param_numel = 0
      for parameter in module.parameters():
        num_param += 1
        if parameter.requires_grad:
          num_param_numel += parameter.numel()
          total_params_element += parameter.numel()
          num_trainable_param += 1

      result.append({'module': module, 'num_param': num_param , 'num_trainable_param' : num_trainable_param, 'num_param_numel': num_param_numel})
  learner.model.apply(check_trainable)
  
  print("{: <85} {: <17} {:,<20} {: <40}".format('Module Name', 'Total Parameters', 'Trainable Parameters', '# Elements in Trainable Parametrs'))
  for row in result:
    print("{: <85} {: <17} {: <20} {: <40,}".format(row['module'].__str__(), row['num_param'], row['num_trainable_param'], row['num_param_numel']))
  print('Total number of parameters elements {:,}'.format(total_params_element))

learn_classifier.freeze()
summary_trainable(learn_classifier)

learn_classifier.freeze()
learn_classifier.freeze_to(-1)
summary_trainable(learn_classifier)

learn_classifier.freeze()
learn_classifier.freeze_to(-2)
summary_trainable(learn_classifier)

learn_classifier.freeze()
learn_classifier.freeze_to(-3)
summary_trainable(learn_classifier)

learn_classifier.freeze()
learn_classifier.freeze_to(-4)
summary_trainable(learn_classifier)

learn_classifier.freeze()
learn_classifier.freeze_to(-5) # equal to unfreeze
summary_trainable(learn_classifier)

learn_classifier.freeze()
learn_classifier.unfreeze()
summary_trainable(learn_classifier)