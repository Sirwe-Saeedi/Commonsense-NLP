# -*- coding: utf-8 -*-
"""Task_C_GPT_2 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IV0Lz-hvad2txaW4VsGzyDHpkxhRK9tE

Conditional text generation with the auto-regressive models of the library (GPT/GPT-2/CTRL/Transformer-XL/XLNet)

#Transformer
"""

!pip install transformers -q

#%load_ext tensorboard

"""#Dataset

##Training set
"""

import pandas as pd
import numpy as np

# Load the dataset into a pandas dataframe.
df = pd.read_csv('subtaskC_data_all.csv', error_bad_lines=False)
df_answers = pd.read_csv('subtaskC_answers_all.csv', error_bad_lines=False, names=['reason1', 'reason2' , 'reason3'])

for i in range(len(df)):
    if df['FalseSent'][i][-1] != '.':
      df['FalseSent'][i] = df['FalseSent'][i] +'.'
      
# Report the number of sentences.
print('Number of training sentences: {:,}\n'.format(df.shape[0]))

# Display 10 random rows from the data.
df.head(10)
df.to_csv('df.csv')
df_answers.to_csv('df_answers.csv')
df_answers

"""##trial set"""

import pandas as pd
import numpy as np

# Load the dataset into a pandas dataframe.
df_trial = pd.read_csv('taskC_trial_data.csv', error_bad_lines=False)
df_trial_answers = pd.read_csv('taskC_trial_references.csv', error_bad_lines=False, header=None, names=['reason1', 'reason2' , 'reason3'])
df_trial_answers.reset_index(drop=True, inplace=True)

for i in range(len(df_trial)):
    if df_trial['FalseSent'][i][-1] != '.':
      df_trial['FalseSent'][i] = df_trial['FalseSent'][i] +'.'
      
# Report the number of sentences.
print('Number of training sentences: {:,}\n'.format(df_trial.shape[0]))

# Display 10 random rows from the data.
df_trial.head(10)
df_trial.to_csv('df_trial.csv')
df_trial_answers.to_csv('df_trial_answers.csv')
df_trial
#df_trial_answers
#df_trial_answers['reason1'][0]

"""##trianing + trial

##Dev set
"""

import pandas as pd
import numpy as np

# Load the dataset into a pandas dataframe.
df_dev = pd.read_csv('subtaskC_dev_data.csv', error_bad_lines=False)
df_dev_answers = pd.read_csv('subtaskC_gold_answers.csv', error_bad_lines=False, header=None, names=['reason1', 'reason2' , 'reason3'])

for i in range(len(df_dev)):
    if df_dev['FalseSent'][i][-1] != '.':
       df_dev['FalseSent'][i] = df_dev['FalseSent'][i] +'.'
      
# Report the number of sentences.
print('Number of training sentences: {:,}\n'.format(df_dev.shape[0]))

# Display 10 random rows from the data.
df_dev.to_csv('df_dev.csv')
df_dev_answers.to_csv('df_dev_answers.csv')

df_dev_answers

"""##Test set"""

import pandas as pd
import numpy as np

# Load the dataset into a pandas dataframe.
df_test = pd.read_csv('subtaskC_test_data.csv', error_bad_lines=False)
df_test.index = np.arange(1, len(df_test) + 1)
for i in range(1, len(df_test)+1):
    if df_test['FalseSent'][i][-1] != '.':
      df_test['FalseSent'][i] = df_test['FalseSent'][i] +'.'

df_test.to_csv('df_test.csv')
df_test

df_test.to_csv('df_test.csv')
#from google.colab import files
#files.download("df_test.csv")

"""#Import Library"""

import argparse
import logging

import numpy as np
import torch

from transformers import (
    CTRLLMHeadModel,
    CTRLTokenizer,
    GPT2LMHeadModel,
    GPT2Tokenizer,
    OpenAIGPTLMHeadModel,
    OpenAIGPTTokenizer,
    TransfoXLLMHeadModel,
    TransfoXLTokenizer,
    XLMTokenizer,
    XLMWithLMHeadModel,
    XLNetLMHeadModel,
    XLNetTokenizer,
)


logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s", datefmt="%m/%d/%Y %H:%M:%S", level=logging.INFO,
)
logger = logging.getLogger(__name__)

MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop

MODEL_CLASSES = {
    "gpt2": (GPT2LMHeadModel, GPT2Tokenizer),
    "gpt2-xl" : (GPT2LMHeadModel, GPT2Tokenizer),
    "ctrl": (CTRLLMHeadModel, CTRLTokenizer),
    "openai-gpt": (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),
    "xlnet": (XLNetLMHeadModel, XLNetTokenizer),
    "transfo-xl": (TransfoXLLMHeadModel, TransfoXLTokenizer),
    "xlm": (XLMWithLMHeadModel, XLMTokenizer),
}

# Padding text to help Transformer-XL and XLNet with short prompts as proposed by Aman Rusia
# in https://github.com/rusiaaman/XLNet-gen#methodology
# and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e
PADDING_TEXT = """In 1991, the remains of Russian Tsar Nicholas II and his family
(except for Alexei and Maria) are discovered.
The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the
remainder of the story. 1883 Western Siberia,
a young Grigori Rasputin is asked by his father and a group of men to perform magic.
Rasputin has a vision and denounces one of the men as a horse thief. Although his
father initially slaps him for making such an accusation, Rasputin watches as the
man is chased outside and beaten. Twenty years later, Rasputin sees a vision of
the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,
with people, even a bishop, begging for his blessing. <eod> </s> <eos>"""


def set_seed(args):
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if args.n_gpu > 0:
        torch.cuda.manual_seed_all(args.seed)

"""# Functions to prepare models' input

##CTRL
"""

def prepare_ctrl_input(args, _, tokenizer, prompt_text):
    if args.temperature > 0.7:
        logger.info("CTRL typically works better with lower temperatures (and lower top_k).")

    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False)
    if not any(encoded_prompt[0] == x for x in tokenizer.control_codes.values()):
        logger.info("WARNING! You are not starting your generation from a control code so you won't get good results")
    return prompt_text

"""##XLM"""

def prepare_xlm_input(args, model, tokenizer, prompt_text):
    # kwargs = {"language": None, "mask_token_id": None}

    # Set the language
    use_lang_emb = hasattr(model.config, "use_lang_emb") and model.config.use_lang_emb
    if hasattr(model.config, "lang2id") and use_lang_emb:
        available_languages = model.config.lang2id.keys()
        if args.xlm_language in available_languages:
            language = args.xlm_language
        else:
            language = None
            while language not in available_languages:
                language = input("Using XLM. Select language in " + str(list(available_languages)) + " >>> ")

        model.config.lang_id = model.config.lang2id[language]
        # kwargs["language"] = tokenizer.lang2id[language]

    # TODO fix mask_token_id setup when configurations will be synchronized between models and tokenizers
    # XLM masked-language modeling (MLM) models need masked token
    # is_xlm_mlm = "mlm" in args.model_name_or_path
    # if is_xlm_mlm:
    #     kwargs["mask_token_id"] = tokenizer.mask_token_id

    return prompt_text

"""##XLnet"""

def prepare_xlnet_input(args, _, tokenizer, prompt_text):
    prompt_text = (args.padding_text if args.padding_text else PADDING_TEXT) + prompt_text
    return prompt_text

"""##transformerXL"""

def prepare_transfoxl_input(args, _, tokenizer, prompt_text):
    prompt_text = (args.padding_text if args.padding_text else PADDING_TEXT) + prompt_text
    return prompt_text

"""## prepare"""

PREPROCESSING_FUNCTIONS = {
    "ctrl": prepare_ctrl_input,
    "xlm": prepare_xlm_input,
    "xlnet": prepare_xlnet_input,
    "transfo-xl": prepare_transfoxl_input,
}


def adjust_length_to_model(length, max_sequence_length):
    if length < 0 and max_sequence_length > 0:
        length = max_sequence_length
    elif 0 < max_sequence_length < length:
        length = max_sequence_length  # No generation bigger than model size
    elif length < 0:
        length = MAX_LENGTH  # avoid infinite loop
    return length

"""##Argument Parser"""

parser = argparse.ArgumentParser()
parser.add_argument(
    "--model_type",
    default="gpt2-xl",
    type=str,
    help="Model type selected in the list: " + ", ".join(MODEL_CLASSES.keys()),
)
parser.add_argument(
    "--model_name_or_path",
    default="gpt2-xl",
    type=str,
    help="Path to pre-trained model or shortcut name selected in the list: " + ", ".join(MODEL_CLASSES.keys()),
)

parser.add_argument("--prompt", type=str, default=" ")
parser.add_argument("--length", type=int, default=20)
parser.add_argument("--stop_token", type=str, default=".", help="Token at which text generation is stopped")

parser.add_argument(
    "--temperature",
    type=float,
    default=1.0,
    help="temperature of 1.0 has no effect, lower tend toward greedy sampling",
)
parser.add_argument(
    "--repetition_penalty", type=float, default=1.0, help="primarily useful for CTRL model; in that case, use 1.2"
)
parser.add_argument("--k", type=int, default=0)
parser.add_argument("--p", type=float, default=0.9)

parser.add_argument("--padding_text", type=str, default="", help="Padding text for Transfo-XL and XLNet.")
parser.add_argument("--xlm_language", type=str, default="", help="Optional language when used with the XLM model.")

parser.add_argument("--seed", type=int, default=42, help="random seed for initialization")
parser.add_argument("--no_cuda", action="store_true", help="Avoid using CUDA when available")
parser.add_argument("--num_return_sequences", type=int, default=1, help="The number of samples to generate.")
parser.add_argument("-f", type=str, help="colab option.")

args = parser.parse_args()

args.device = torch.device("cuda" if torch.cuda.is_available() and not args.no_cuda else "cpu")
args.n_gpu = torch.cuda.device_count()

set_seed(args)

"""#Initialize the model and tokenizer"""

# Initialize the model and tokenizer
try:
    args.model_type = args.model_type.lower()
    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]
except KeyError:
    raise KeyError("the model {} you specified is not supported. You are welcome to add it and open a PR :)")

tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)
model = model_class.from_pretrained(args.model_name_or_path)
model.to(args.device)

args.length = adjust_length_to_model(args.length, max_sequence_length=model.config.max_position_embeddings)
logger.info(args)

prompt_text = args.prompt if args.prompt else input("Model prompt >>> ")

"""# Remove the batch dimension when returning multiple sequences"""

def run(prompt_text):
  # Different models need different input formatting and/or extra arguments
  requires_preprocessing = args.model_type in PREPROCESSING_FUNCTIONS.keys()
  if requires_preprocessing:
      prepare_input = PREPROCESSING_FUNCTIONS.get(args.model_type)
      preprocessed_prompt_text = prepare_input(args, model, tokenizer, prompt_text)
      encoded_prompt = tokenizer.encode(
          preprocessed_prompt_text, add_special_tokens=False, return_tensors="pt", add_space_before_punct_symbol=True
      )
  else:
      encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors="pt")
  encoded_prompt = encoded_prompt.to(args.device)

  output_sequences = model.generate(
      input_ids=encoded_prompt,
      max_length=args.length + len(encoded_prompt[0]),
      temperature=args.temperature,
      top_k=args.k,
      top_p=args.p,
      repetition_penalty=args.repetition_penalty,
      do_sample=True,
      num_return_sequences=args.num_return_sequences,
  )

  if len(output_sequences.shape) > 2:
      output_sequences.squeeze_()

  generated_sequences = []

  for generated_sequence_idx, generated_sequence in enumerate(output_sequences):
      # print("=== GENERATED SEQUENCE {} ===".format(generated_sequence_idx + 1))
      generated_sequence = generated_sequence.tolist()

      # Decode text
      text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)

      # Remove all text after the stop token
      text = text[: text.find(args.stop_token) if args.stop_token else None]

      # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing
      total_sequence = (
          prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]
      )

      generated_sequences.append(total_sequence)
  return generated_sequences

run('The sentence "He drinks apple" doesn\'t make sense because')

"""#BLEU Score"""

from typing import List, Dict
import csv
import logging
import sys
import argparse
import collections
import math

EXIT_STATUS_ANSWERS_MALFORMED = 1
EXIT_STATUS_PREDICTIONS_MALFORMED = 2
EXIT_STATUS_PREDICTIONS_EXTRA = 3
EXIT_STATUS_PREDICTION_MISSING = 4


def _get_ngrams(segment, max_order):
    """Extracts all n-grams upto a given maximum order from an input segment.
    Args:
        segment: text segment from which n-grams will be extracted.
        max_order: maximum length in tokens of the n-grams returned by this
        methods.
    Returns:
        The Counter containing all n-grams upto max_order in segment
        with a count of how many times each n-gram occurred.
    """
    ngram_counts = collections.Counter()
    for order in range(1, max_order + 1):
        for i in range(0, len(segment) - order + 1):
            ngram = tuple(segment[i:i+order])
            ngram_counts[ngram] += 1
    return ngram_counts


def _compute_bleu(reference_corpus, translation_corpus, max_order=4, smooth=False):
    """Computes BLEU score of translated segments against one or more references.
    Args:
        reference_corpus: list of lists of references for each translation. Each
            reference should be tokenized into a list of tokens.
        translation_corpus: list of translations to score. Each translation
            should be tokenized into a list of tokens.
        max_order: Maximum n-gram order to use when computing BLEU score.
        smooth: Whether or not to apply Lin et al. 2004 smoothing.
    Returns:
        3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram
            precisions and brevity penalty.
    """
    matches_by_order = [0] * max_order
    possible_matches_by_order = [0] * max_order
    reference_length = 0
    translation_length = 0
    for (references, translation) in zip(reference_corpus, translation_corpus):
        reference_length += min(len(r) for r in references)
        translation_length += len(translation)

        merged_ref_ngram_counts = collections.Counter()
        for reference in references:
            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)
        translation_ngram_counts = _get_ngrams(translation, max_order)
        overlap = translation_ngram_counts & merged_ref_ngram_counts
        for ngram in overlap:
            matches_by_order[len(ngram)-1] += overlap[ngram]
        for order in range(1, max_order+1):
            possible_matches = len(translation) - order + 1
            if possible_matches > 0:
                possible_matches_by_order[order-1] += possible_matches

    precisions = [0] * max_order
    for i in range(0, max_order):
        if smooth:
            precisions[i] = ((matches_by_order[i] + 1.) /
                             (possible_matches_by_order[i] + 1.))
        else:
            if possible_matches_by_order[i] > 0:
                precisions[i] = (float(matches_by_order[i]) /
                                 possible_matches_by_order[i])
            else:
                precisions[i] = 0.0

    if min(precisions) > 0:
        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)
        geo_mean = math.exp(p_log_sum)
    else:
        geo_mean = 0

    ratio = float(translation_length) / reference_length

    if ratio > 1.0:
        bp = 1.
    else:
        bp = math.exp(1 - 1. / ratio)

    bleu = geo_mean * bp

    return (bleu, precisions, bp, ratio, translation_length, reference_length)


def calculate_bleu(references: Dict[str, List[List[str]]],
                   predictions: Dict[str, List[str]],
                   max_order=4,
                   smooth=False) -> float:

    reference_corpus = []
    prediction_corpus = []

    for instance_id, reference_sents in references.items():
        try:
            prediction_sent = predictions[instance_id]
        except KeyError:
            logging.error("Missing prediction for instance '%s'.", instance_id)
            sys.exit(EXIT_STATUS_PREDICTION_MISSING)

        del predictions[instance_id]

        prediction_corpus.append(prediction_sent)
        reference_corpus.append(reference_sents)

    if len(predictions) > 0:
        logging.error("Found %d extra predictions, for example: %s", len(predictions),
                      ", ".join(list(predictions.keys())[:3]))
        sys.exit(EXIT_STATUS_PREDICTIONS_EXTRA)

    score = _compute_bleu(reference_corpus, prediction_corpus,
                          max_order=max_order, smooth=smooth)[0]

    return score


def read_references(filename: str) -> List[List[List[str]]]:
    references = {}
    with open(filename, "rt", encoding="UTF-8", errors="replace") as f:
        reader = csv.reader(f)
        try:
            for row in reader:
                try:
                    instance_id = row[0]
                    references_raw1 = row[1]
                    references_raw2 = row[2]
                    references_raw3 = row[3]
                except IndexError as e:
                    logging.error(
                        "Error reading value from CSV file %s on line %d: %s", filename, reader.line_num, e)
                    sys.exit(EXIT_STATUS_ANSWERS_MALFORMED)

                if instance_id in references:
                    logging.error("Key %s repeated in file %s on line %d",
                                  instance_id, filename, reader.line_num)
                    sys.exit(EXIT_STATUS_ANSWERS_MALFORMED)

                if instance_id == "":
                    logging.error(
                        "Key is empty in file %s on line %d", filename, reader.line_num)
                    sys.exit(EXIT_STATUS_ANSWERS_MALFORMED)

                tokens = []
                for ref in [references_raw1, references_raw2, references_raw3]:
                    if ref:
                        tokens.append(ref.split())

                if len(tokens) == 0:
                    logging.error(
                        "No reference sentence in file %s on line %d", filename, reader.line_num)
                    sys.exit(EXIT_STATUS_ANSWERS_MALFORMED)

                references[instance_id] = tokens

        except csv.Error as e:
            logging.error('file %s, line %d: %s', filename, reader.line_num, e)
            sys.exit(EXIT_STATUS_ANSWERS_MALFORMED)

    return references


def read_predictions(filename: str) -> List[List[str]]:
    predictions = {}
    with open(filename, "rt", encoding="UTF-8", errors="replace") as f:
        reader = csv.reader(f)
        try:
            for row in reader:
                try:
                    instance_id = row[0]
                    prediction_raw = row[1]
                except IndexError as e:
                    logging.error(
                        "Error reading value from CSV file %s on line %d: %s", filename, reader.line_num, e)
                    sys.exit(EXIT_STATUS_PREDICTIONS_MALFORMED)

                if instance_id in predictions:
                    logging.error("Key %s repeated in file %s on line %d",
                                  instance_id, filename, reader.line_num)
                    sys.exit(EXIT_STATUS_PREDICTIONS_MALFORMED)

                if instance_id == "":
                    logging.error(
                        "Key is empty in file %s on line %d", filename, reader.line_num)
                    sys.exit(EXIT_STATUS_PREDICTIONS_MALFORMED)

                if prediction_raw == "":
                    logging.warning("Key % s has empty prediction in file % s on line % d",
                                    instance_id, filename, reader.line_num)

                tokens = prediction_raw.split()
                predictions[instance_id] = tokens

        except csv.Error as e:
            logging.error('file %s, line %d: %s', filename, reader.line_num, e)
            sys.exit(EXIT_STATUS_PREDICTIONS_MALFORMED)

    return predictions


def main():
    references = read_references(args2.references)
    predictions = read_predictions(args2.predictions)

    bleu = calculate_bleu(references, predictions,
                          max_order=args2.max_order, smooth=args2.smooth)

    print(f'BLEU score: {bleu*100:.4f}.')


if __name__ == '__main__':
    parser2 = argparse.ArgumentParser(
        description='SemEval 2020 Task 4 subtask C official evaluation script')
    parser2.add_argument('--references', '-r',
                        help='reference file in csv format')
    parser2.add_argument('--predictions', '-p',
                        help='prediction file in csv format')
    parser2.add_argument(
        '--max_order', default=4, type=int, help='Maximum n-gram order to use when computing BLEU score')
    parser2.add_argument('--smooth', action='store_true',
                        help='Whether or not to apply Lin et al. 2004 smoothing')
    parser2.add_argument("-f", type=str, help="colab option.")
    args2 = parser2.parse_args()

references = {}
predictions = {}

for i in range(1, len(df_test)+1): 
  #references[i] = [df_trial_answers['reason1'][i].split(), df_trial_answers['reason2'][i].split(), df_trial_answers['reason3'][i].split()]
  sent = df_test['FalseSent'][i]
  #predictions[i] = df['FalseSent'][i].split()
  predictions[i] = run(sent)[0].split()

predictions[1]

List = []
List2= []
for i in range(1 , len(df_test)+1):
  List.append(predictions[i])
  List2.append(' '.join(word for word in List[i-1]))

List2

submission = pd.DataFrame(List2) 
submission.index = np.arange(1, len(submission) + 1)
submission

submission.to_csv('subtaskC_answers.csv', header=False)
from google.colab import files
files.download('subtaskC_answers.csv')

